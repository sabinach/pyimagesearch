{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"30-deeper-googlenet-tiny-imagenet.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMPH7UqKOoVBTKCfT1plPC7"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"46fbsGcALXKO"},"source":["**Use GPU: Runtime -> Change runtime type -> GPU (Hardware Accelerator)**"]},{"cell_type":"markdown","metadata":{"id":"Ccx5-tofLZoe"},"source":["Setup"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I_1Xd8PYLYsA","executionInfo":{"status":"ok","timestamp":1618903684912,"user_tz":240,"elapsed":1284,"user":{"displayName":"Sabina Chen","photoUrl":"","userId":"13519457631091889537"}},"outputId":"5d00d8a7-f836-4ac4-bf5c-d7cecdb361e0"},"source":["!cat ~/.keras/keras.json"],"execution_count":null,"outputs":[{"output_type":"stream","text":["{\n","    \"epsilon\": 1e-07, \n","    \"floatx\": \"float32\", \n","    \"image_data_format\": \"channels_last\", \n","    \"backend\": \"tensorflow\"\n","}"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9_fi6VeSLcGM","executionInfo":{"status":"ok","timestamp":1618903686766,"user_tz":240,"elapsed":3133,"user":{"displayName":"Sabina Chen","photoUrl":"","userId":"13519457631091889537"}},"outputId":"08e828fc-600e-4d55-f60f-ef359e21e70e"},"source":["import keras\n","print(keras.__version__)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["2.4.3\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"TgXjCkbhLiCh"},"source":["---\n","Tiny ImageNet Config"]},{"cell_type":"code","metadata":{"id":"CLfoPIrHLdHH"},"source":["# import the necessary packages\n","from os import path\n","\n","# define the paths to the training and validation directories\n","TRAIN_IMAGES = \"drive/MyDrive/pyimagesearch/datasets/tiny-imagenet-200/train\"\n","VAL_IMAGES = \"drive/MyDrive/pyimagesearch/datasets/tiny-imagenet-200/val/images\"\n","\n","# define the path to the file that maps validation filenames to\n","# their corresponding class labels\n","VAL_MAPPINGS = \"drive/MyDrive/pyimagesearch/datasets/tiny-imagenet-200/val/val_annotations.txt\"\n","\n","# define the paths to the WordNet hierarchy files which are used\n","# to generate our class labels\n","WORDNET_IDS = \"drive/MyDrive/pyimagesearch/datasets/tiny-imagenet-200/wnids.txt\"\n","WORD_LABELS = \"drive/MyDrive/pyimagesearch/datasets/tiny-imagenet-200/words.txt\"\n","\n","# since we do not have access to the testing data we need to\n","# take a number of images from the training data and use it instead\n","NUM_CLASSES = 200\n","NUM_TEST_IMAGES = 50 * NUM_CLASSES\n","\n","# define the path to the output training, validation, and testing HDF5 files\n","TRAIN_HDF5 = \"drive/MyDrive/pyimagesearch/output/30-deeper-googlenet-tiny-imagenet/hdf5/train.hdf5\"\n","VAL_HDF5 = \"drive/MyDrive/pyimagesearch/output/30-deeper-googlenet-tiny-imagenet/hdf5/val.hdf5\"\n","TEST_HDF5 = \"drive/MyDrive/pyimagesearch/output/30-deeper-googlenet-tiny-imagenet/hdf5/test.hdf5\"\n","\n","# define the path to the dataset mean\n","DATASET_MEAN = \"drive/MyDrive/pyimagesearch/output/30-deeper-googlenet-tiny-imagenet/tiny-image-net-200-mean.json\"\n","\n","# define the path to the output directory used for storing plots, classification reports, etc.\n","OUTPUT_PATH = \"drive/MyDrive/pyimagesearch/output/30-deeper-googlenet-tiny-imagenet/\"\n","MODEL_PATH = path.sep.join([OUTPUT_PATH, \"deepergooglenet_tinyimagenet.hdf5\"])\n","FIG_PATH = path.sep.join([OUTPUT_PATH, \"deepergooglenet_tinyimagenet.png\"])\n","JSON_PATH = path.sep.join([OUTPUT_PATH, \"deepergooglenet_tinyimagenet.json\"])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zFwGXDplNLbK"},"source":["---\n","HDF5 Dataset Writer"]},{"cell_type":"code","metadata":{"id":"Ixgr8ktlNNGl"},"source":["# import the necessary packages\n","import h5py\n","import os\n","\n","class HDF5DatasetWriter:\n","\tdef __init__(self, dims, outputPath, dataKey=\"images\",\n","\t\tbufSize=1000):\n","\t\t# check to see if the output path exists, and if so, raise\n","\t\t# an exception\n","\t\tif os.path.exists(outputPath):\n","\t\t\traise ValueError(\"The supplied `outputPath` already \"\n","\t\t\t\t\"exists and cannot be overwritten. Manually delete \"\n","\t\t\t\t\"the file before continuing.\", outputPath)\n","\n","\t\t# open the HDF5 database for writing and create two datasets:\n","\t\t# one to store the images/features and another to store the\n","\t\t# class labels\n","\t\tself.db = h5py.File(outputPath, \"w\")\n","\t\tself.data = self.db.create_dataset(dataKey, dims,\n","\t\t\tdtype=\"float\")\n","\t\tself.labels = self.db.create_dataset(\"labels\", (dims[0],),\n","\t\t\tdtype=\"int\")\n","\n","\t\t# store the buffer size, then initialize the buffer itself\n","\t\t# along with the index into the datasets\n","\t\tself.bufSize = bufSize\n","\t\tself.buffer = {\"data\": [], \"labels\": []}\n","\t\tself.idx = 0\n","\n","\tdef add(self, rows, labels):\n","\t\t# add the rows and labels to the buffer\n","\t\tself.buffer[\"data\"].extend(rows)\n","\t\tself.buffer[\"labels\"].extend(labels)\n","\n","\t\t# check to see if the buffer needs to be flushed to disk\n","\t\tif len(self.buffer[\"data\"]) >= self.bufSize:\n","\t\t\tself.flush()\n","\n","\tdef flush(self):\n","\t\t# write the buffers to disk then reset the buffer\n","\t\ti = self.idx + len(self.buffer[\"data\"])\n","\t\tself.data[self.idx:i] = self.buffer[\"data\"]\n","\t\tself.labels[self.idx:i] = self.buffer[\"labels\"]\n","\t\tself.idx = i\n","\t\tself.buffer = {\"data\": [], \"labels\": []}\n","\n","\tdef storeClassLabels(self, classLabels):\n","\t\t# create a dataset to store the actual class label names,\n","\t\t# then store the class labels\n","\t\tdt = h5py.special_dtype(vlen=str) # `vlen=unicode` for Py2.7\n","\t\tlabelSet = self.db.create_dataset(\"label_names\",\n","\t\t\t(len(classLabels),), dtype=dt)\n","\t\tlabelSet[:] = classLabels\n","\n","\tdef close(self):\n","\t\t# check to see if there are any other entries in the buffer\n","\t\t# that need to be flushed to disk\n","\t\tif len(self.buffer[\"data\"]) > 0:\n","\t\t\tself.flush()\n","\n","\t\t# close the dataset\n","\t\tself.db.close()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"usbLFdL2MuKM"},"source":["Build TinyImageNet (convert to HDF5)"]},{"cell_type":"code","metadata":{"id":"NhkZLAU-Mtrl"},"source":["# import the necessary packages\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.model_selection import train_test_split\n","from imutils import paths\n","import numpy as np\n","import progressbar\n","import json\n","import cv2\n","import os"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TqRAdGTuX28E"},"source":["def build_tinyimagenet():\n","    # grab the paths to the training images, then extract the training\n","    # class labels and encode them\n","    trainPaths = list(paths.list_images(config.TRAIN_IMAGES))\n","    trainLabels = [p.split(os.path.sep)[-3] for p in trainPaths]\n","    le = LabelEncoder()\n","    trainLabels = le.fit_transform(trainLabels)\n","\n","    # perform stratified sampling from the training set to construct a\n","    # a testing set\n","    split = train_test_split(trainPaths, trainLabels,\n","        test_size=config.NUM_TEST_IMAGES, stratify=trainLabels,\n","        random_state=42)\n","    (trainPaths, testPaths, trainLabels, testLabels) = split\n","\n","    # load the validation filename => class from file and then use these\n","    # mappings to build the validation paths and label lists\n","    M = open(config.VAL_MAPPINGS).read().strip().split(\"\\n\")\n","    M = [r.split(\"\\t\")[:2] for r in M]\n","    valPaths = [os.path.sep.join([config.VAL_IMAGES, m[0]]) for m in M]\n","    valLabels = le.transform([m[1] for m in M])\n","\n","    # construct a list pairing the training, validation, and testing\n","    # image paths along with their corresponding labels and output HDF5\n","    # files\n","    datasets = [\n","        (\"train\", trainPaths, trainLabels, config.TRAIN_HDF5),\n","        (\"val\", valPaths, valLabels, config.VAL_HDF5),\n","        (\"test\", testPaths, testLabels, config.TEST_HDF5)]\n","\n","    # initialize the lists of RGB channel averages\n","    (R, G, B) = ([], [], [])\n","\n","    # loop over the dataset tuples\n","    for (dType, paths, labels, outputPath) in datasets:\n","        # create HDF5 writer\n","        print(\"[INFO] building {}...\".format(outputPath))\n","        writer = HDF5DatasetWriter((len(paths), 64, 64, 3), outputPath)\n","\n","        # initialize the progress bar\n","        widgets = [\"Building Dataset: \", progressbar.Percentage(), \" \", progressbar.Bar(), \" \", progressbar.ETA()]\n","        pbar = progressbar.ProgressBar(maxval=len(paths), widgets=widgets).start()\n","\n","        # loop over the image paths\n","        for (i, (path, label)) in enumerate(zip(paths, labels)):\n","            # load the image from disk\n","            image = cv2.imread(path)\n","\n","            # if we are building the training dataset, then compute the\n","            # mean of each channel in the image, then update the\n","            # respective lists\n","            if dType == \"train\":\n","                (b, g, r) = cv2.mean(image)[:3]\n","                R.append(r)\n","                G.append(g)\n","                B.append(b)\n","\n","            # add the image and label to the HDF5 dataset\n","            writer.add([image], [label])\n","            pbar.update(i)\n","\n","        # close the HDF5 writer\n","        pbar.finish()\n","        writer.close()\n","\n","    # construct a dictionary of averages, then serialize the means to a\n","    # JSON file\n","    print(\"[INFO] serializing means...\")\n","    D = {\"R\": np.mean(R), \"G\": np.mean(G), \"B\": np.mean(B)}\n","    f = open(config.DATASET_MEAN, \"w\")\n","    f.write(json.dumps(D))\n","    f.close()\n","    print(\"[INFO] done.\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XyU8QwaGYBYM"},"source":["build_tinyimagenet()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cbpguH6EN8wb"},"source":["---\n","Preprocessing"]},{"cell_type":"code","metadata":{"id":"hbJVpR7TOF4V"},"source":["# import the necessary packages\n","import numpy as np\n","import cv2\n","from sklearn.feature_extraction.image import extract_patches_2d\n","from keras.preprocessing.image import img_to_array\n","\n","class SimplePreprocessor:\n","    def __init__(self, width, height, inter=cv2.INTER_AREA):\n","        # store the target image width, height, and interpolation\n","        # method used when resizing\n","        self.width = width\n","        self.height = height\n","        self.inter = inter\n","\n","    def preprocess(self, image):\n","        # resize the image to a fixed size, ignoring the aspect\n","        # ratio\n","        return cv2.resize(image, (self.width, self.height), interpolation=self.inter)\n","\n","class PatchPreprocessor:\n","    def __init__(self, width, height):\n","        # store the target width and height of the image\n","        self.width = width\n","        self.height = height\n","\n","    def preprocess(self, image):\n","        # extract a random crop from the image with the target width\n","        # and height\n","        return extract_patches_2d(image, (self.height, self.width), max_patches=1)[0]\n","\n","class MeanPreprocessor:\n","    def __init__(self, rMean, gMean, bMean):\n","        # store the Red, Green, and Blue channel averages across a\n","        # training set\n","        self.rMean = rMean\n","        self.gMean = gMean\n","        self.bMean = bMean\n","\n","    def preprocess(self, image):\n","        # split the image into its respective Red, Green, and Blue\n","        # channels\n","        (B, G, R) = cv2.split(image.astype(\"float32\"))\n","\n","        # subtract the means for each channel\n","        R -= self.rMean\n","        G -= self.gMean\n","        B -= self.bMean\n","\n","        # merge the channels back together and return the image\n","        return cv2.merge([B, G, R])\n","\n","class ImageToArrayPreprocessor:\n","    def __init__(self, dataFormat=None):\n","        # store the image data format\n","        self.dataFormat = dataFormat\n","\n","    def preprocess(self, image):\n","        # apply the Keras utility function that correctly rearranges\n","        # the dimensions of the image\n","        return img_to_array(image, data_format=self.dataFormat)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hYoEDPjmOGAy"},"source":["Training Monitor"]},{"cell_type":"code","metadata":{"id":"GrG6n6kPOHgO"},"source":["# import the necessary packages\n","from keras.callbacks import BaseLogger\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import json\n","import os\n","\n","class TrainingMonitor(BaseLogger):\n","    def __init__(self, figPath, jsonPath=None, startAt=0):\n","        # store the output path for the figure, the path to the JSON\n","        # serialized file, and the starting epoch\n","        super(TrainingMonitor, self).__init__()\n","        self.figPath = figPath\n","        self.jsonPath = jsonPath\n","        self.startAt = startAt\n","\n","    def on_train_begin(self, logs={}):\n","        # initialize the history dictionary\n","        self.H = {}\n","\n","        # if the JSON history path exists, load the training history\n","        if self.jsonPath is not None:\n","            if os.path.exists(self.jsonPath):\n","                self.H = json.loads(open(self.jsonPath).read())\n","\n","                # check to see if a starting epoch was supplied\n","                if self.startAt > 0:\n","                    # loop over the entries in the history log and\n","                    # trim any entries that are past the starting\n","                    # epoch\n","                    for k in self.H.keys():\n","                        self.H[k] = self.H[k][:self.startAt]\n","\n","    def on_epoch_end(self, epoch, logs={}):\n","        # loop over the logs and update the loss, accuracy, etc.\n","        # for the entire training process\n","        for (k, v) in logs.items():\n","            l = self.H.get(k, [])\n","            l.append(float(v))\n","            self.H[k] = l\n","\n","        # check to see if the training history should be serialized\n","        # to file\n","        if self.jsonPath is not None:\n","            f = open(self.jsonPath, \"w\")\n","            f.write(json.dumps(self.H))\n","            f.close()\n","\n","        # ensure at least two epochs have passed before plotting\n","        # (epoch starts at zero)\n","        if len(self.H[\"loss\"]) > 1:\n","            # plot the training loss and accuracy\n","            N = np.arange(0, len(self.H[\"loss\"]))\n","            plt.figure()\n","            plt.plot(N, self.H[\"loss\"], label=\"train_loss\")\n","            plt.plot(N, self.H[\"val_loss\"], label=\"val_loss\")\n","            plt.plot(N, self.H[\"accuracy\"], label=\"train_accuracy\")\n","            plt.plot(N, self.H[\"val_accuracy\"], label=\"val_accuracy\")\n","            plt.title(\"Training Loss and Accuracy [Epoch {}]\".format(len(self.H[\"loss\"])))\n","            plt.xlabel(\"Epoch #\")\n","            plt.ylabel(\"Loss/Accuracy\")\n","            plt.legend()\n","\n","            # save the figure\n","            plt.savefig(self.figPath)\n","            plt.close()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nsUTXBFZOuFn"},"source":["Epoch Checkpoint"]},{"cell_type":"code","metadata":{"id":"Tkw00APZOuPo"},"source":["# import the necessary packages\n","from keras.callbacks import Callback\n","import os\n","\n","class EpochCheckpoint(Callback):\n","\tdef __init__(self, outputPath, every=5, startAt=0):\n","\t\t# call the parent constructor\n","\t\tsuper(Callback, self).__init__()\n","\n","\t\t# store the base output path for the model, the number of\n","\t\t# epochs that must pass before the model is serialized to\n","\t\t# disk and the current epoch value\n","\t\tself.outputPath = outputPath\n","\t\tself.every = every\n","\t\tself.intEpoch = startAt\n","\n","\tdef on_epoch_end(self, epoch, logs={}):\n","\t\t# check to see if the model should be serialized to disk\n","\t\tif (self.intEpoch + 1) % self.every == 0:\n","\t\t\tp = os.path.sep.join([self.outputPath,\n","\t\t\t\t\"epoch_{}.hdf5\".format(self.intEpoch + 1)])\n","\t\t\tself.model.save(p, overwrite=True)\n","\n","\t\t# increment the internal epoch counter\n","\t\tself.intEpoch += 1"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XdkNhf9lOHpE"},"source":["HDF5 Dataset Generator"]},{"cell_type":"code","metadata":{"id":"i-9gr4ErORaj"},"source":["# import the necessary packages\n","from keras.utils import np_utils\n","import numpy as np\n","import h5py\n","\n","class HDF5DatasetGenerator:\n","    def __init__(self, dbPath, batchSize, preprocessors=None,\n","        aug=None, binarize=True, classes=2):\n","        # store the batch size, preprocessors, and data augmentor,\n","        # whether or not the labels should be binarized, along with\n","        # the total number of classes\n","        self.batchSize = batchSize\n","        self.preprocessors = preprocessors\n","        self.aug = aug\n","        self.binarize = binarize\n","        self.classes = classes\n","\n","        # open the HDF5 database for reading and determine the total\n","        # number of entries in the database\n","        self.db = h5py.File(dbPath)\n","        self.numImages = self.db[\"labels\"].shape[0]\n","\n","    def generator(self, passes=np.inf):\n","        # initialize the epoch count\n","        epochs = 0\n","\n","        # keep looping infinitely -- the model will stop once we have\n","        # reach the desired number of epochs\n","        while epochs < passes:\n","            # loop over the HDF5 dataset\n","            for i in np.arange(0, self.numImages, self.batchSize):\n","                # extract the images and labels from the HDF dataset\n","                images = self.db[\"images\"][i: i + self.batchSize]\n","                labels = self.db[\"labels\"][i: i + self.batchSize]\n","\n","                # check to see if the labels should be binarized\n","                if self.binarize:\n","                    labels = np_utils.to_categorical(labels,\n","                        self.classes)\n","\n","                # check to see if our preprocessors are not None\n","                if self.preprocessors is not None:\n","                    # initialize the list of processed images\n","                    procImages = []\n","\n","                    # loop over the images\n","                    for image in images:\n","                        # loop over the preprocessors and apply each\n","                        # to the image\n","                        for p in self.preprocessors:\n","                            image = p.preprocess(image)\n","\n","                        # update the list of processed images\n","                        procImages.append(image)\n","\n","                    # update the images array to be the processed\n","                    # images\n","                    images = np.array(procImages)\n","\n","                # if the data augmenator exists, apply it\n","                if self.aug is not None:\n","                    (images, labels) = next(self.aug.flow(images,\n","                        labels, batch_size=self.batchSize))\n","\n","                # yield a tuple of images and labels\n","                yield (images, labels)\n","\n","            # increment the total number of epochs\n","            epochs += 1\n","\n","    def close(self):\n","        # close the database\n","        self.db.close()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Hexjp9pVOS4t"},"source":["Deeper GoogLeNet"]},{"cell_type":"code","metadata":{"id":"3kyTEDFJOSwj"},"source":["# import the necessary packages\n","from keras.layers.normalization import BatchNormalization\n","from keras.layers.convolutional import Conv2D\n","from keras.layers.convolutional import AveragePooling2D\n","from keras.layers.convolutional import MaxPooling2D\n","from keras.layers.core import Activation\n","from keras.layers.core import Dropout\n","from keras.layers.core import Dense\n","from keras.layers import Flatten\n","from keras.layers import Input\n","from keras.models import Model\n","from keras.layers import concatenate\n","from keras.regularizers import l2\n","from keras import backend as K\n","\n","class DeeperGoogLeNet:\n","\t@staticmethod\n","\tdef conv_module(x, K, kX, kY, stride, chanDim,\n","\t\tpadding=\"same\", reg=0.0005, name=None):\n","\t\t# initialize the CONV, BN, and RELU layer names\n","\t\t(convName, bnName, actName) = (None, None, None)\n","\n","\t\t# if a layer name was supplied, prepend it\n","\t\tif name is not None:\n","\t\t\tconvName = name + \"_conv\"\n","\t\t\tbnName = name + \"_bn\"\n","\t\t\tactName = name + \"_act\"\n","\n","\t\t# define a CONV => BN => RELU pattern\n","\t\tx = Conv2D(K, (kX, kY), strides=stride, padding=padding, kernel_regularizer=l2(reg), name=convName)(x)\n","\t\tx = BatchNormalization(axis=chanDim, name=bnName)(x)\n","\t\tx = Activation(\"relu\", name=actName)(x)\n","\n","\t\t# return the block\n","\t\treturn x\n","\n","\t@staticmethod\n","\tdef inception_module(x, num1x1, num3x3Reduce, num3x3,\n","\t\tnum5x5Reduce, num5x5, num1x1Proj, chanDim, stage,\n","\t\treg=0.0005):\n","\t\t# define the first branch of the Inception module which\n","\t\t# consists of 1x1 convolutions\n","\t\tfirst = DeeperGoogLeNet.conv_module(x, num1x1, 1, 1, (1, 1), chanDim, reg=reg, name=stage + \"_first\")\n","\n","\t\t# define the second branch of the Inception module which\n","\t\t# consists of 1x1 and 3x3 convolutions\n","\t\tsecond = DeeperGoogLeNet.conv_module(x, num3x3Reduce, 1, 1, (1, 1), chanDim, reg=reg, name=stage + \"_second1\")\n","\t\tsecond = DeeperGoogLeNet.conv_module(second, num3x3, 3, 3, (1, 1), chanDim, reg=reg, name=stage + \"_second2\")\n","\n","\t\t# define the third branch of the Inception module which\n","\t\t# are our 1x1 and 5x5 convolutions\n","\t\tthird = DeeperGoogLeNet.conv_module(x, num5x5Reduce, 1, 1, (1, 1), chanDim, reg=reg, name=stage + \"_third1\")\n","\t\tthird = DeeperGoogLeNet.conv_module(third, num5x5, 5, 5, (1, 1), chanDim, reg=reg, name=stage + \"_third2\")\n","\n","\t\t# define the fourth branch of the Inception module which\n","\t\t# is the POOL projection\n","\t\tfourth = MaxPooling2D((3, 3), strides=(1, 1), padding=\"same\", name=stage + \"_pool\")(x)\n","\t\tfourth = DeeperGoogLeNet.conv_module(fourth, num1x1Proj, 1, 1, (1, 1), chanDim, reg=reg, name=stage + \"_fourth\")\n","\n","\t\t# concatenate across the channel dimension\n","\t\tx = concatenate([first, second, third, fourth], axis=chanDim, name=stage + \"_mixed\")\n","\n","\t\t# return the block\n","\t\treturn x\n","\n","\t@staticmethod\n","\tdef build(width, height, depth, classes, reg=0.0005):\n","\t\t# initialize the input shape to be \"channels last\" and the\n","\t\t# channels dimension itself\n","\t\tinputShape = (height, width, depth)\n","\t\tchanDim = -1\n","\n","\t\t# if we are using \"channels first\", update the input shape\n","\t\t# and channels dimension\n","\t\tif K.image_data_format() == \"channels_first\":\n","\t\t\tinputShape = (depth, height, width)\n","\t\t\tchanDim = 1\n","\n","\t\t# define the model input, followed by a sequence of CONV =>\n","\t\t# POOL => (CONV * 2) => POOL layers\n","\t\tinputs = Input(shape=inputShape)\n","\t\tx = DeeperGoogLeNet.conv_module(inputs, 64, 5, 5, (1, 1), chanDim, reg=reg, name=\"block1\")\n","\t\tx = MaxPooling2D((3, 3), strides=(2, 2), padding=\"same\", name=\"pool1\")(x)\n","\t\tx = DeeperGoogLeNet.conv_module(x, 64, 1, 1, (1, 1), chanDim, reg=reg, name=\"block2\")\n","\t\tx = DeeperGoogLeNet.conv_module(x, 192, 3, 3, (1, 1), chanDim, reg=reg, name=\"block3\")\n","\t\tx = MaxPooling2D((3, 3), strides=(2, 2), padding=\"same\", name=\"pool2\")(x)\n","\n","\t\t# apply two Inception modules followed by a POOL\n","\t\tx = DeeperGoogLeNet.inception_module(x, 64, 96, 128, 16, 32, 32, chanDim, \"3a\", reg=reg)\n","\t\tx = DeeperGoogLeNet.inception_module(x, 128, 128, 192, 32, 96, 64, chanDim, \"3b\", reg=reg)\n","\t\tx = MaxPooling2D((3, 3), strides=(2, 2), padding=\"same\", name=\"pool3\")(x)\n","\n","\t\t# apply five Inception modules followed by POOL\n","\t\tx = DeeperGoogLeNet.inception_module(x, 192, 96, 208, 16, 48, 64, chanDim, \"4a\", reg=reg)\n","\t\tx = DeeperGoogLeNet.inception_module(x, 160, 112, 224, 24, 64, 64, chanDim, \"4b\", reg=reg)\n","\t\tx = DeeperGoogLeNet.inception_module(x, 128, 128, 256, 24, 64, 64, chanDim, \"4c\", reg=reg)\n","\t\tx = DeeperGoogLeNet.inception_module(x, 112, 144, 288, 32, 64, 64, chanDim, \"4d\", reg=reg)\n","\t\tx = DeeperGoogLeNet.inception_module(x, 256, 160, 320, 32, 128, 128, chanDim, \"4e\", reg=reg)\n","\t\tx = MaxPooling2D((3, 3), strides=(2, 2), padding=\"same\", name=\"pool4\")(x)\n","\n","\t\t# apply a POOL layer (average) followed by dropout\n","\t\tx = AveragePooling2D((4, 4), name=\"pool5\")(x)\n","\t\tx = Dropout(0.4, name=\"do\")(x)\n","\n","\t\t# softmax classifier\n","\t\tx = Flatten(name=\"flatten\")(x)\n","\t\tx = Dense(classes, kernel_regularizer=l2(reg), name=\"labels\")(x)\n","\t\tx = Activation(\"softmax\", name=\"softmax\")(x)\n","\n","\t\t# create the model\n","\t\tmodel = Model(inputs, x, name=\"googlenet\")\n","\n","\t\t# return the constructed network architecture\n","\t\treturn model"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SnLWt30wMy4f"},"source":["Train GoogLeNet on HDF5"]},{"cell_type":"code","metadata":{"id":"21QPuVcqODCd"},"source":["# import the necessary packages\n","from keras.preprocessing.image import ImageDataGenerator\n","from keras.optimizers import Adam\n","from keras.models import load_model\n","import keras.backend as K\n","import json"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nf6bM-qrPu0G"},"source":["def train_googlenet_tinyimage(checkpoint_folder, model_filepath, start_epoch=0):\n","    # construct the training image generator for data augmentation\n","    aug = ImageDataGenerator(rotation_range=18, zoom_range=0.15,\n","        width_shift_range=0.2, height_shift_range=0.2, shear_range=0.15,\n","        horizontal_flip=True, fill_mode=\"nearest\")\n","\n","    # load the RGB means for the training set\n","    means = json.loads(open(config.DATASET_MEAN).read())\n","\n","    # initialize the image preprocessors\n","    sp = SimplePreprocessor(64, 64)\n","    mp = MeanPreprocessor(means[\"R\"], means[\"G\"], means[\"B\"])\n","    iap = ImageToArrayPreprocessor()\n","\n","    # initialize the training and validation dataset generators\n","    trainGen = HDF5DatasetGenerator(config.TRAIN_HDF5, 64, aug=aug, preprocessors=[sp, mp, iap], classes=config.NUM_CLASSES)\n","    valGen = HDF5DatasetGenerator(config.VAL_HDF5, 64, preprocessors=[sp, mp, iap], classes=config.NUM_CLASSES)\n","\n","    # if there is no specific model checkpoint supplied, then initialize\n","    # the network and compile the model\n","    if args[\"model\"] is None:\n","        print(\"[INFO] compiling model...\")\n","        model = DeeperGoogLeNet.build(width=64, height=64, depth=3, classes=config.NUM_CLASSES, reg=0.0002)\n","        opt = Adam(1e-3)\n","        model.compile(loss=\"categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n","\n","    # otherwise, load the checkpoint from disk\n","    else:\n","        print(\"[INFO] loading {}...\".format(args[\"model\"]))\n","        model = load_model(args[\"model\"])\n","\n","        # update the learning rate\n","        print(\"[INFO] old learning rate: {}\".format(K.get_value(model.optimizer.lr)))\n","        K.set_value(model.optimizer.lr, 1e-5)\n","        print(\"[INFO] new learning rate: {}\".format(K.get_value(model.optimizer.lr)))\n","\n","    # construct the set of callbacks\n","    callbacks = [\n","        EpochCheckpoint(args[\"checkpoints\"], every=5, startAt=args[\"start_epoch\"]),\n","        TrainingMonitor(config.FIG_PATH, jsonPath=config.JSON_PATH, startAt=args[\"start_epoch\"])]\n","\n","    # train the network\n","    model.fit_generator(\n","        trainGen.generator(),\n","        steps_per_epoch=trainGen.numImages // 64,\n","        validation_data=valGen.generator(),\n","        validation_steps=valGen.numImages // 64,\n","        epochs=10,\n","        max_queue_size=10,\n","        callbacks=callbacks, verbose=1)\n","\n","    # close the databases\n","    trainGen.close()\n","    valGen.close()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ENrmwV9PP720"},"source":["train_googlenet_tinyimage(checkpoint_folder=\"drive/MyDrive/pyimagesearch/output/30-deeper-googlenet-tiny-imagenet/checkpoints/\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MG2On8FSRKbJ"},"source":["train_googlenet_tinyimage(checkpoint_folder=\"drive/MyDrive/pyimagesearch/output/30-deeper-googlenet-tiny-imagenet/checkpoints/\", \n","                          model_filepath=\"drive/MyDrive/pyimagesearch/output/30-deeper-googlenet-tiny-imagenet/checkpoints/epoch_25.hdf5\", \n","                          start_epoch=25)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ApKAY9xUN-v_"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"EXpEtVdlM5mP"},"source":["Rank-5 Accuracy"]},{"cell_type":"code","metadata":{"id":"sbVJCQbyM7k9"},"source":["import numpy as np\n","\n","def rank5_accuracy(preds, labels):\n","\t# initialize the rank-1 and rank-5 accuracies\n","\trank1 = 0\n","\trank5 = 0\n","\n","\t# loop over the predictions and ground-truth labels\n","\tfor (p, gt) in zip(preds, labels):\n","\t\t# sort the probabilities by their index in descending\n","\t\t# order so that the more confident guesses are at the\n","\t\t# front of the list\n","\t\tp = np.argsort(p)[::-1]\n","\n","\t\t# check if the ground-truth label is in the top-5\n","\t\t# predictions\n","\t\tif gt in p[:5]:\n","\t\t\trank5 += 1\n","\n","\t\t# check to see if the ground-truth is the #1 prediction\n","\t\tif gt == p[0]:\n","\t\t\trank1 += 1\n","\n","\t# compute the final rank-1 and rank-5 accuracies\n","\trank1 /= float(len(preds))\n","\trank5 /= float(len(preds))\n","\n","\t# return a tuple of the rank-1 and rank-5 accuracies\n","\treturn (rank1, rank5)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xtrshtGiPP-n"},"source":["Rank Accuracy"]},{"cell_type":"code","metadata":{"id":"qfeoVTD8PPcm"},"source":["def rank_accuracy():\n","    # load the RGB means for the training set\n","    means = json.loads(open(DATASET_MEAN).read())\n","\n","    # initialize the image preprocessors\n","    sp = SimplePreprocessor(64, 64)\n","    mp = MeanPreprocessor(means[\"R\"], means[\"G\"], means[\"B\"])\n","    iap = ImageToArrayPreprocessor()\n","\n","    # initialize the testing dataset generator\n","    testGen = HDF5DatasetGenerator(TEST_HDF5, 64, preprocessors=[sp, mp, iap], classes=NUM_CLASSES)\n","\n","    # load the pre-trained network\n","    print(\"[INFO] loading model...\")\n","    model = load_model(MODEL_PATH)\n","\n","    # make predictions on the testing data\n","    print(\"[INFO] predicting on test data...\")\n","    predictions = model.predict_generator(testGen.generator(), steps=testGen.numImages // 64, max_queue_size=10)\n","\n","    # compute the rank-1 and rank-5 accuracies\n","    (rank1, rank5) = rank5_accuracy(predictions, testGen.db[\"labels\"])\n","    print(\"[INFO] rank-1: {:.2f}%\".format(rank1 * 100))\n","    print(\"[INFO] rank-5: {:.2f}%\".format(rank5 * 100))\n","\n","    # close the database\n","    testGen.close()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jyvmOn5DPmsd"},"source":["rank_accuracy()"],"execution_count":null,"outputs":[]}]}