{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"24-fine-tuning.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1iQcBD7fFujmvO7W_Ugj1S6_Q7XU2c589","authorship_tag":"ABX9TyM75a291i4DrrvD9fB7itSp"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"d6wB_sX6uUg8"},"source":["**Use GPU: Runtime -> Change runtime type -> GPU (Hardware Accelerator)**"]},{"cell_type":"markdown","metadata":{"id":"HGZxLspFrTYz"},"source":["Setup"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WF_5t_JWrWf2","executionInfo":{"status":"ok","timestamp":1618812778956,"user_tz":240,"elapsed":631,"user":{"displayName":"Sabina Chen","photoUrl":"","userId":"13519457631091889537"}},"outputId":"dc339649-2416-44f5-c04b-0b7dab591b38"},"source":["!cat ~/.keras/keras.json"],"execution_count":null,"outputs":[{"output_type":"stream","text":["{\n","    \"epsilon\": 1e-07, \n","    \"floatx\": \"float32\", \n","    \"image_data_format\": \"channels_last\", \n","    \"backend\": \"tensorflow\"\n","}"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"e9IX8feHrfdx"},"source":["FCHeadNet"]},{"cell_type":"code","metadata":{"id":"8-d_acDqrYJg"},"source":["# import the necessary packages\n","from keras.layers.core import Dropout\n","from keras.layers.core import Flatten\n","from keras.layers.core import Dense\n","\n","class FCHeadNet:\n","\t@staticmethod\n","\tdef build(baseModel, classes, D):\n","\t\t# initialize the head model that will be placed on top of\n","\t\t# the base, then add a FC layer\n","\t\theadModel = baseModel.output\n","\t\theadModel = Flatten(name=\"flatten\")(headModel)\n","\t\theadModel = Dense(D, activation=\"relu\")(headModel)\n","\t\theadModel = Dropout(0.5)(headModel)\n","\n","\t\t# add a softmax layer\n","\t\theadModel = Dense(classes, activation=\"softmax\")(headModel)\n","\n","\t\t# return the model\n","\t\treturn headModel"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5LFXBCc8sG5O"},"source":["Preprocessing"]},{"cell_type":"code","metadata":{"id":"OhvGTQiHrhIr"},"source":["import numpy as np\n","import imutils\n","import cv2\n","import os\n","from keras.preprocessing.image import img_to_array\n","\n","class AspectAwarePreprocessor:\n","  def __init__(self, width, height, inter=cv2.INTER_AREA):\n","    # store the target image width, height, and interpolation\n","    # method used when resizing\n","    self.width = width\n","    self.height = height\n","    self.inter = inter\n","\n","  def preprocess(self, image):\n","    # grab the dimensions of the image and then initialize\n","    # the deltas to use when cropping\n","    (h, w) = image.shape[:2]\n","    dW = 0\n","    dH = 0\n","\n","    # if the width is smaller than the height, then resize\n","    # along the width (i.e., the smaller dimension) and then\n","    # update the deltas to crop the height to the desired\n","    # dimension\n","    if w < h:\n","      image = imutils.resize(image, width=self.width,\n","        inter=self.inter)\n","      dH = int((image.shape[0] - self.height) / 2.0)\n","\n","    # otherwise, the height is smaller than the width so\n","    # resize along the height and then update the deltas\n","    # crop along the width\n","    else:\n","      image = imutils.resize(image, height=self.height,\n","        inter=self.inter)\n","      dW = int((image.shape[1] - self.width) / 2.0)\n","\n","    # now that our images have been resized, we need to\n","    # re-grab the width and height, followed by performing\n","    # the crop\n","    (h, w) = image.shape[:2]\n","    image = image[dH:h - dH, dW:w - dW]\n","\n","    # finally, resize the image to the provided spatial\n","    # dimensions to ensure our output image is always a fixed\n","    # size\n","    return cv2.resize(image, (self.width, self.height),\n","      interpolation=self.inter)\n","\n","class ImageToArrayPreprocessor:\n","\tdef __init__(self, dataFormat=None):\n","\t\t# store the image data format\n","\t\tself.dataFormat = dataFormat\n","\n","\tdef preprocess(self, image):\n","\t\t# apply the Keras utility function that correctly rearranges\n","\t\t# the dimensions of the image\n","\t\treturn img_to_array(image, data_format=self.dataFormat)\n","  \n","class SimpleDatasetLoader:\n","    def __init__(self, preprocessors=None):\n","        # store the image preprocessor\n","        self.preprocessors = preprocessors\n","\n","        # if the preprocessors are None, initialize them as an empty list\n","        if self.preprocessors is None:\n","            self.preprocessors = []\n","\n","    def load(self, imagePaths, verbose=-1):\n","        # initialize the list of features and labels\n","        data = []\n","        labels = []\n","\n","        # loop over the input images\n","        for (i, imagePath) in enumerate(imagePaths):\n","            # load the image and extract the class label assuming\n","            # that our path has the following format:\n","            # /path/to/dataset/{class}/{image}.jpg\n","            image = cv2.imread(imagePath)\n","            label = imagePath.split(os.path.sep)[-2]\n","\n","            # check to see if our preprocessors are not None\n","            if self.preprocessors is not None:\n","                # loop over the preprocessors and apply each to\n","                # the image\n","                for p in self.preprocessors:\n","                    image = p.preprocess(image)\n","\n","            # treat our processed image as a \"feature vector\"\n","            # by updating the data list followed by the labels\n","            data.append(image)\n","            labels.append(label)\n","\n","            # show an update every `verbose` images\n","            if verbose > 0 and i > 0 and (i + 1) % verbose == 0:\n","                print(\"[INFO] processed {}/{}\".format(i + 1,\n","                    len(imagePaths)))\n","\n","        # return a tuple of the data and labels\n","        return (np.array(data), np.array(labels))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tqWZCN2wsRRn"},"source":["Fine Tune (Flowers17)"]},{"cell_type":"code","metadata":{"id":"-a_gcEVisSrn"},"source":["# import the necessary packages\n","from sklearn.preprocessing import LabelBinarizer\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report\n","from keras.preprocessing.image import ImageDataGenerator\n","from keras.optimizers import RMSprop\n","from keras.optimizers import SGD\n","from keras.applications import VGG16\n","from keras.layers import Input\n","from keras.models import Model\n","from imutils import paths\n","import numpy as np\n","import os"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SoOpgcT9sVTq"},"source":["def finetune_flowers17(dataset_filepath, model_filepath):\n","    # construct the image generator for data augmentation\n","    aug = ImageDataGenerator(rotation_range=30, width_shift_range=0.1,\n","        height_shift_range=0.1, shear_range=0.2, zoom_range=0.2,\n","        horizontal_flip=True, fill_mode=\"nearest\")\n","\n","    # grab the list of images that we'll be describing, then extract\n","    # the class label names from the image paths\n","    print(\"[INFO] loading images...\")\n","    imagePaths = list(paths.list_images(dataset_filepath))\n","    classNames = [pt.split(os.path.sep)[-2] for pt in imagePaths]\n","    classNames = [str(x) for x in np.unique(classNames)]\n","\n","    # initialize the image preprocessors\n","    aap = AspectAwarePreprocessor(224, 224)\n","    iap = ImageToArrayPreprocessor()\n","\n","    # load the dataset from disk then scale the raw pixel intensities to\n","    # the range [0, 1]\n","    sdl = SimpleDatasetLoader(preprocessors=[aap, iap])\n","    (data, labels) = sdl.load(imagePaths, verbose=500)\n","    data = data.astype(\"float\") / 255.0\n","\n","    # partition the data into training and testing splits using 75% of\n","    # the data for training and the remaining 25% for testing\n","    (trainX, testX, trainY, testY) = train_test_split(data, labels, test_size=0.25, random_state=42)\n","\n","    # convert the labels from integers to vectors\n","    trainY = LabelBinarizer().fit_transform(trainY)\n","    testY = LabelBinarizer().fit_transform(testY)\n","\n","    # load the VGG16 network, ensuring the head FC layer sets are left\n","    # off\n","    baseModel = VGG16(weights=\"imagenet\", include_top=False, input_tensor=Input(shape=(224, 224, 3)))\n","\n","    # initialize the new head of the network, a set of FC layers\n","    # followed by a softmax classifier\n","    headModel = FCHeadNet.build(baseModel, len(classNames), 256)\n","\n","    # place the head FC model on top of the base model -- this will\n","    # become the actual model we will train\n","    model = Model(inputs=baseModel.input, outputs=headModel)\n","\n","    # loop over all layers in the base model and freeze them so they\n","    # will *not* be updated during the training process\n","    for layer in baseModel.layers:\n","        layer.trainable = False\n","\n","    # compile our model (this needs to be done after our setting our\n","    # layers to being non-trainable\n","    print(\"[INFO] compiling model...\")\n","    opt = RMSprop(lr=0.001)\n","    model.compile(loss=\"categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n","\n","    # train the head of the network for a few epochs (all other\n","    # layers are frozen) -- this will allow the new FC layers to\n","    # start to become initialized with actual \"learned\" values\n","    # versus pure random\n","    print(\"[INFO] training head...\")\n","    model.fit_generator(aug.flow(trainX, trainY, batch_size=32),\n","        validation_data=(testX, testY), epochs=25,\n","        steps_per_epoch=len(trainX) // 32, verbose=1)\n","\n","    # evaluate the network after initialization\n","    print(\"[INFO] evaluating after initialization...\")\n","    predictions = model.predict(testX, batch_size=32)\n","    print(classification_report(testY.argmax(axis=1), predictions.argmax(axis=1), target_names=classNames))\n","\n","    # now that the head FC layers have been trained/initialized, lets\n","    # unfreeze the final set of CONV layers and make them trainable\n","    for layer in baseModel.layers[15:]:\n","        layer.trainable = True\n","\n","    # for the changes to the model to take affect we need to recompile\n","    # the model, this time using SGD with a *very* small learning rate\n","    print(\"[INFO] re-compiling model...\")\n","    opt = SGD(lr=0.001)\n","    model.compile(loss=\"categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n","\n","    # train the model again, this time fine-tuning *both* the final set\n","    # of CONV layers along with our set of FC layers\n","    print(\"[INFO] fine-tuning model...\")\n","    model.fit_generator(aug.flow(trainX, trainY, batch_size=32),\n","        validation_data=(testX, testY), epochs=100,\n","        steps_per_epoch=len(trainX) // 32, verbose=1)\n","\n","    # evaluate the network on the fine-tuned model\n","    print(\"[INFO] evaluating after fine-tuning...\")\n","    predictions = model.predict(testX, batch_size=32)\n","    print(classification_report(testY.argmax(axis=1), predictions.argmax(axis=1), target_names=classNames))\n","\n","    # save the model to disk\n","    print(\"[INFO] serializing model...\")\n","    model.save(model_filepath) "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EUwB7wJlsvNj","executionInfo":{"status":"ok","timestamp":1618815617189,"user_tz":240,"elapsed":2834890,"user":{"displayName":"Sabina Chen","photoUrl":"","userId":"13519457631091889537"}},"outputId":"ff456b57-2d1b-4ece-8529-9c278038c5f4"},"source":["finetune_flowers17(dataset_filepath=\"drive/MyDrive/pyimagesearch/datasets/flowers17\", model_filepath=\"drive/MyDrive/pyimagesearch/output/21-feature-extraction/flowers17_features.hdf5\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[INFO] loading images...\n","[INFO] processed 500/1360\n","[INFO] processed 1000/1360\n","Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n","58892288/58889256 [==============================] - 1s 0us/step\n","[INFO] compiling model...\n","[INFO] training head...\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n","  warnings.warn('`Model.fit_generator` is deprecated and '\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch 1/25\n","31/31 [==============================] - 71s 1s/step - loss: 7.1930 - accuracy: 0.1094 - val_loss: 2.1805 - val_accuracy: 0.3618\n","Epoch 2/25\n","31/31 [==============================] - 17s 534ms/step - loss: 2.4440 - accuracy: 0.2188 - val_loss: 1.7940 - val_accuracy: 0.4324\n","Epoch 3/25\n","31/31 [==============================] - 17s 533ms/step - loss: 2.1436 - accuracy: 0.3024 - val_loss: 1.3311 - val_accuracy: 0.6176\n","Epoch 4/25\n","31/31 [==============================] - 17s 532ms/step - loss: 1.7449 - accuracy: 0.4203 - val_loss: 1.2707 - val_accuracy: 0.6176\n","Epoch 5/25\n","31/31 [==============================] - 17s 531ms/step - loss: 1.7457 - accuracy: 0.4289 - val_loss: 1.0973 - val_accuracy: 0.6353\n","Epoch 6/25\n","31/31 [==============================] - 17s 530ms/step - loss: 1.6052 - accuracy: 0.4906 - val_loss: 1.0338 - val_accuracy: 0.7118\n","Epoch 7/25\n","31/31 [==============================] - 17s 530ms/step - loss: 1.3792 - accuracy: 0.5645 - val_loss: 0.7916 - val_accuracy: 0.7912\n","Epoch 8/25\n","31/31 [==============================] - 17s 531ms/step - loss: 1.2917 - accuracy: 0.5826 - val_loss: 0.5836 - val_accuracy: 0.8206\n","Epoch 9/25\n","31/31 [==============================] - 16s 530ms/step - loss: 1.1583 - accuracy: 0.5979 - val_loss: 0.7636 - val_accuracy: 0.7676\n","Epoch 10/25\n","31/31 [==============================] - 16s 529ms/step - loss: 1.1659 - accuracy: 0.6269 - val_loss: 0.6374 - val_accuracy: 0.8324\n","Epoch 11/25\n","31/31 [==============================] - 16s 529ms/step - loss: 1.0444 - accuracy: 0.6665 - val_loss: 0.5281 - val_accuracy: 0.8382\n","Epoch 12/25\n","31/31 [==============================] - 16s 530ms/step - loss: 1.0532 - accuracy: 0.6747 - val_loss: 0.4579 - val_accuracy: 0.8500\n","Epoch 13/25\n","31/31 [==============================] - 16s 527ms/step - loss: 1.0666 - accuracy: 0.6333 - val_loss: 0.4685 - val_accuracy: 0.8559\n","Epoch 14/25\n","31/31 [==============================] - 16s 527ms/step - loss: 1.0097 - accuracy: 0.6683 - val_loss: 0.4816 - val_accuracy: 0.8647\n","Epoch 15/25\n","31/31 [==============================] - 16s 525ms/step - loss: 0.9412 - accuracy: 0.6700 - val_loss: 0.4142 - val_accuracy: 0.8618\n","Epoch 16/25\n","31/31 [==============================] - 16s 527ms/step - loss: 0.8589 - accuracy: 0.7202 - val_loss: 0.4218 - val_accuracy: 0.8794\n","Epoch 17/25\n","31/31 [==============================] - 16s 528ms/step - loss: 0.8365 - accuracy: 0.7251 - val_loss: 0.4394 - val_accuracy: 0.8588\n","Epoch 18/25\n","31/31 [==============================] - 16s 525ms/step - loss: 0.7725 - accuracy: 0.7202 - val_loss: 0.4192 - val_accuracy: 0.8588\n","Epoch 19/25\n","31/31 [==============================] - 16s 527ms/step - loss: 0.8604 - accuracy: 0.7172 - val_loss: 0.3999 - val_accuracy: 0.8647\n","Epoch 20/25\n","31/31 [==============================] - 16s 522ms/step - loss: 0.8773 - accuracy: 0.7192 - val_loss: 0.3849 - val_accuracy: 0.8794\n","Epoch 21/25\n","31/31 [==============================] - 17s 530ms/step - loss: 0.8484 - accuracy: 0.7241 - val_loss: 0.4680 - val_accuracy: 0.8588\n","Epoch 22/25\n","31/31 [==============================] - 16s 525ms/step - loss: 0.7902 - accuracy: 0.7173 - val_loss: 0.4711 - val_accuracy: 0.8618\n","Epoch 23/25\n","31/31 [==============================] - 16s 527ms/step - loss: 0.7478 - accuracy: 0.7461 - val_loss: 0.3985 - val_accuracy: 0.8824\n","Epoch 24/25\n","31/31 [==============================] - 17s 531ms/step - loss: 0.6961 - accuracy: 0.7677 - val_loss: 0.3936 - val_accuracy: 0.8735\n","Epoch 25/25\n","31/31 [==============================] - 17s 531ms/step - loss: 0.7005 - accuracy: 0.7560 - val_loss: 0.4049 - val_accuracy: 0.8676\n","[INFO] evaluating after initialization...\n","              precision    recall  f1-score   support\n","\n","    bluebell       0.94      0.80      0.86        20\n","   buttercup       0.94      0.89      0.92        19\n","   coltsfoot       0.80      0.80      0.80        15\n","     cowslip       0.78      0.86      0.82        21\n","      crocus       0.68      0.89      0.77        19\n","    daffodil       0.87      0.87      0.87        23\n","       daisy       1.00      0.93      0.96        27\n","   dandelion       1.00      0.80      0.89        20\n","  fritillary       1.00      0.94      0.97        16\n","        iris       1.00      0.91      0.95        22\n","  lilyvalley       0.84      1.00      0.91        16\n","       pansy       0.90      0.95      0.93        20\n","    snowdrop       0.81      0.65      0.72        20\n","   sunflower       0.95      1.00      0.97        18\n","   tigerlily       1.00      1.00      1.00        18\n","       tulip       0.53      0.74      0.62        23\n","  windflower       1.00      0.78      0.88        23\n","\n","    accuracy                           0.87       340\n","   macro avg       0.89      0.87      0.87       340\n","weighted avg       0.89      0.87      0.87       340\n","\n","[INFO] re-compiling model...\n","[INFO] fine-tuning model...\n","Epoch 1/100\n","31/31 [==============================] - 18s 559ms/step - loss: 0.6833 - accuracy: 0.7492 - val_loss: 0.3367 - val_accuracy: 0.9000\n","Epoch 2/100\n","31/31 [==============================] - 17s 546ms/step - loss: 0.5838 - accuracy: 0.7873 - val_loss: 0.3070 - val_accuracy: 0.9294\n","Epoch 3/100\n","31/31 [==============================] - 17s 543ms/step - loss: 0.4898 - accuracy: 0.8224 - val_loss: 0.3195 - val_accuracy: 0.9324\n","Epoch 4/100\n","31/31 [==============================] - 17s 539ms/step - loss: 0.4957 - accuracy: 0.8373 - val_loss: 0.3239 - val_accuracy: 0.9000\n","Epoch 5/100\n","31/31 [==============================] - 17s 538ms/step - loss: 0.4073 - accuracy: 0.8559 - val_loss: 0.3266 - val_accuracy: 0.9029\n","Epoch 6/100\n","31/31 [==============================] - 17s 535ms/step - loss: 0.4327 - accuracy: 0.8579 - val_loss: 0.2756 - val_accuracy: 0.9324\n","Epoch 7/100\n","31/31 [==============================] - 17s 537ms/step - loss: 0.4672 - accuracy: 0.8449 - val_loss: 0.2748 - val_accuracy: 0.9324\n","Epoch 8/100\n","31/31 [==============================] - 17s 537ms/step - loss: 0.4317 - accuracy: 0.8356 - val_loss: 0.2651 - val_accuracy: 0.9176\n","Epoch 9/100\n","31/31 [==============================] - 17s 537ms/step - loss: 0.4216 - accuracy: 0.8405 - val_loss: 0.2815 - val_accuracy: 0.9324\n","Epoch 10/100\n","31/31 [==============================] - 17s 537ms/step - loss: 0.4789 - accuracy: 0.8302 - val_loss: 0.2693 - val_accuracy: 0.9235\n","Epoch 11/100\n","31/31 [==============================] - 17s 537ms/step - loss: 0.3755 - accuracy: 0.8747 - val_loss: 0.2835 - val_accuracy: 0.9235\n","Epoch 12/100\n","31/31 [==============================] - 17s 536ms/step - loss: 0.3944 - accuracy: 0.8622 - val_loss: 0.2882 - val_accuracy: 0.9206\n","Epoch 13/100\n","31/31 [==============================] - 17s 537ms/step - loss: 0.4388 - accuracy: 0.8477 - val_loss: 0.3090 - val_accuracy: 0.9176\n","Epoch 14/100\n","31/31 [==============================] - 17s 532ms/step - loss: 0.4297 - accuracy: 0.8355 - val_loss: 0.2878 - val_accuracy: 0.9235\n","Epoch 15/100\n","31/31 [==============================] - 17s 535ms/step - loss: 0.4518 - accuracy: 0.8458 - val_loss: 0.2849 - val_accuracy: 0.9118\n","Epoch 16/100\n","31/31 [==============================] - 17s 533ms/step - loss: 0.3415 - accuracy: 0.8779 - val_loss: 0.2792 - val_accuracy: 0.9206\n","Epoch 17/100\n","31/31 [==============================] - 17s 532ms/step - loss: 0.4184 - accuracy: 0.8523 - val_loss: 0.2914 - val_accuracy: 0.9176\n","Epoch 18/100\n","31/31 [==============================] - 17s 531ms/step - loss: 0.3896 - accuracy: 0.8688 - val_loss: 0.3046 - val_accuracy: 0.9235\n","Epoch 19/100\n","31/31 [==============================] - 17s 532ms/step - loss: 0.3770 - accuracy: 0.8617 - val_loss: 0.2862 - val_accuracy: 0.9294\n","Epoch 20/100\n","31/31 [==============================] - 17s 533ms/step - loss: 0.3460 - accuracy: 0.8695 - val_loss: 0.2785 - val_accuracy: 0.9118\n","Epoch 21/100\n","31/31 [==============================] - 17s 531ms/step - loss: 0.3731 - accuracy: 0.8744 - val_loss: 0.2945 - val_accuracy: 0.9118\n","Epoch 22/100\n","31/31 [==============================] - 16s 529ms/step - loss: 0.3205 - accuracy: 0.9069 - val_loss: 0.2711 - val_accuracy: 0.9294\n","Epoch 23/100\n","31/31 [==============================] - 17s 534ms/step - loss: 0.3872 - accuracy: 0.8512 - val_loss: 0.3232 - val_accuracy: 0.9206\n","Epoch 24/100\n","31/31 [==============================] - 17s 530ms/step - loss: 0.4016 - accuracy: 0.8568 - val_loss: 0.2982 - val_accuracy: 0.9265\n","Epoch 25/100\n","31/31 [==============================] - 17s 533ms/step - loss: 0.3036 - accuracy: 0.9009 - val_loss: 0.2900 - val_accuracy: 0.9176\n","Epoch 26/100\n","31/31 [==============================] - 16s 528ms/step - loss: 0.3410 - accuracy: 0.8773 - val_loss: 0.2798 - val_accuracy: 0.9206\n","Epoch 27/100\n","31/31 [==============================] - 17s 533ms/step - loss: 0.3405 - accuracy: 0.8704 - val_loss: 0.2972 - val_accuracy: 0.9265\n","Epoch 28/100\n","31/31 [==============================] - 17s 537ms/step - loss: 0.3449 - accuracy: 0.8755 - val_loss: 0.2758 - val_accuracy: 0.9235\n","Epoch 29/100\n","31/31 [==============================] - 17s 539ms/step - loss: 0.3473 - accuracy: 0.8693 - val_loss: 0.2683 - val_accuracy: 0.9265\n","Epoch 30/100\n","31/31 [==============================] - 17s 534ms/step - loss: 0.3703 - accuracy: 0.8793 - val_loss: 0.2916 - val_accuracy: 0.9265\n","Epoch 31/100\n","31/31 [==============================] - 17s 533ms/step - loss: 0.3570 - accuracy: 0.8694 - val_loss: 0.2726 - val_accuracy: 0.9176\n","Epoch 32/100\n","31/31 [==============================] - 17s 537ms/step - loss: 0.3223 - accuracy: 0.8791 - val_loss: 0.2576 - val_accuracy: 0.9353\n","Epoch 33/100\n","31/31 [==============================] - 17s 535ms/step - loss: 0.3242 - accuracy: 0.8868 - val_loss: 0.2561 - val_accuracy: 0.9265\n","Epoch 34/100\n","31/31 [==============================] - 16s 530ms/step - loss: 0.2744 - accuracy: 0.9039 - val_loss: 0.2707 - val_accuracy: 0.9324\n","Epoch 35/100\n","31/31 [==============================] - 17s 534ms/step - loss: 0.3320 - accuracy: 0.8908 - val_loss: 0.2566 - val_accuracy: 0.9353\n","Epoch 36/100\n","31/31 [==============================] - 16s 526ms/step - loss: 0.2993 - accuracy: 0.8861 - val_loss: 0.2647 - val_accuracy: 0.9324\n","Epoch 37/100\n","31/31 [==============================] - 17s 532ms/step - loss: 0.3033 - accuracy: 0.8819 - val_loss: 0.2935 - val_accuracy: 0.9265\n","Epoch 38/100\n","31/31 [==============================] - 17s 540ms/step - loss: 0.2735 - accuracy: 0.9026 - val_loss: 0.2935 - val_accuracy: 0.9382\n","Epoch 39/100\n","31/31 [==============================] - 17s 538ms/step - loss: 0.2967 - accuracy: 0.8877 - val_loss: 0.2826 - val_accuracy: 0.9147\n","Epoch 40/100\n","31/31 [==============================] - 17s 545ms/step - loss: 0.3238 - accuracy: 0.8845 - val_loss: 0.2623 - val_accuracy: 0.9294\n","Epoch 41/100\n","31/31 [==============================] - 17s 543ms/step - loss: 0.3019 - accuracy: 0.8930 - val_loss: 0.2992 - val_accuracy: 0.9235\n","Epoch 42/100\n","31/31 [==============================] - 17s 540ms/step - loss: 0.2989 - accuracy: 0.8858 - val_loss: 0.2917 - val_accuracy: 0.9235\n","Epoch 43/100\n","31/31 [==============================] - 17s 539ms/step - loss: 0.2484 - accuracy: 0.9128 - val_loss: 0.2697 - val_accuracy: 0.9294\n","Epoch 44/100\n","31/31 [==============================] - 17s 530ms/step - loss: 0.2890 - accuracy: 0.8971 - val_loss: 0.2683 - val_accuracy: 0.9353\n","Epoch 45/100\n","31/31 [==============================] - 17s 533ms/step - loss: 0.3045 - accuracy: 0.9035 - val_loss: 0.2699 - val_accuracy: 0.9353\n","Epoch 46/100\n","31/31 [==============================] - 16s 527ms/step - loss: 0.2550 - accuracy: 0.9204 - val_loss: 0.2976 - val_accuracy: 0.9235\n","Epoch 47/100\n","31/31 [==============================] - 16s 530ms/step - loss: 0.2683 - accuracy: 0.8905 - val_loss: 0.2574 - val_accuracy: 0.9382\n","Epoch 48/100\n","31/31 [==============================] - 16s 530ms/step - loss: 0.2976 - accuracy: 0.9074 - val_loss: 0.2721 - val_accuracy: 0.9353\n","Epoch 49/100\n","31/31 [==============================] - 17s 531ms/step - loss: 0.2660 - accuracy: 0.9151 - val_loss: 0.2698 - val_accuracy: 0.9324\n","Epoch 50/100\n","31/31 [==============================] - 17s 531ms/step - loss: 0.2561 - accuracy: 0.9015 - val_loss: 0.2876 - val_accuracy: 0.9294\n","Epoch 51/100\n","31/31 [==============================] - 17s 535ms/step - loss: 0.2501 - accuracy: 0.9158 - val_loss: 0.2745 - val_accuracy: 0.9324\n","Epoch 52/100\n","31/31 [==============================] - 17s 530ms/step - loss: 0.2173 - accuracy: 0.9202 - val_loss: 0.2763 - val_accuracy: 0.9353\n","Epoch 53/100\n","31/31 [==============================] - 17s 534ms/step - loss: 0.3177 - accuracy: 0.8913 - val_loss: 0.2605 - val_accuracy: 0.9382\n","Epoch 54/100\n","31/31 [==============================] - 17s 533ms/step - loss: 0.2385 - accuracy: 0.9201 - val_loss: 0.2768 - val_accuracy: 0.9412\n","Epoch 55/100\n","31/31 [==============================] - 17s 534ms/step - loss: 0.2856 - accuracy: 0.8958 - val_loss: 0.3074 - val_accuracy: 0.9235\n","Epoch 56/100\n","31/31 [==============================] - 17s 537ms/step - loss: 0.2539 - accuracy: 0.9166 - val_loss: 0.3076 - val_accuracy: 0.9324\n","Epoch 57/100\n","31/31 [==============================] - 17s 536ms/step - loss: 0.2345 - accuracy: 0.9175 - val_loss: 0.2874 - val_accuracy: 0.9265\n","Epoch 58/100\n","31/31 [==============================] - 17s 531ms/step - loss: 0.2579 - accuracy: 0.9149 - val_loss: 0.2730 - val_accuracy: 0.9324\n","Epoch 59/100\n","31/31 [==============================] - 17s 536ms/step - loss: 0.2160 - accuracy: 0.9134 - val_loss: 0.3047 - val_accuracy: 0.9324\n","Epoch 60/100\n","31/31 [==============================] - 17s 532ms/step - loss: 0.2188 - accuracy: 0.9267 - val_loss: 0.2816 - val_accuracy: 0.9265\n","Epoch 61/100\n","31/31 [==============================] - 16s 529ms/step - loss: 0.2451 - accuracy: 0.9092 - val_loss: 0.2767 - val_accuracy: 0.9353\n","Epoch 62/100\n","31/31 [==============================] - 17s 534ms/step - loss: 0.2863 - accuracy: 0.9127 - val_loss: 0.2579 - val_accuracy: 0.9412\n","Epoch 63/100\n","31/31 [==============================] - 17s 533ms/step - loss: 0.2756 - accuracy: 0.8903 - val_loss: 0.2609 - val_accuracy: 0.9353\n","Epoch 64/100\n","31/31 [==============================] - 16s 525ms/step - loss: 0.2577 - accuracy: 0.9033 - val_loss: 0.2688 - val_accuracy: 0.9382\n","Epoch 65/100\n","31/31 [==============================] - 17s 529ms/step - loss: 0.2765 - accuracy: 0.8969 - val_loss: 0.2652 - val_accuracy: 0.9412\n","Epoch 66/100\n","31/31 [==============================] - 17s 531ms/step - loss: 0.2972 - accuracy: 0.8903 - val_loss: 0.2959 - val_accuracy: 0.9412\n","Epoch 67/100\n","31/31 [==============================] - 17s 534ms/step - loss: 0.2172 - accuracy: 0.9295 - val_loss: 0.2730 - val_accuracy: 0.9412\n","Epoch 68/100\n","31/31 [==============================] - 17s 531ms/step - loss: 0.2276 - accuracy: 0.9195 - val_loss: 0.2716 - val_accuracy: 0.9353\n","Epoch 69/100\n","31/31 [==============================] - 17s 536ms/step - loss: 0.2523 - accuracy: 0.9009 - val_loss: 0.2674 - val_accuracy: 0.9353\n","Epoch 70/100\n","31/31 [==============================] - 17s 533ms/step - loss: 0.2439 - accuracy: 0.9094 - val_loss: 0.2730 - val_accuracy: 0.9353\n","Epoch 71/100\n","31/31 [==============================] - 17s 536ms/step - loss: 0.2371 - accuracy: 0.9156 - val_loss: 0.2783 - val_accuracy: 0.9353\n","Epoch 72/100\n","31/31 [==============================] - 16s 529ms/step - loss: 0.2093 - accuracy: 0.9148 - val_loss: 0.2616 - val_accuracy: 0.9382\n","Epoch 73/100\n","31/31 [==============================] - 17s 532ms/step - loss: 0.1948 - accuracy: 0.9176 - val_loss: 0.2630 - val_accuracy: 0.9412\n","Epoch 74/100\n","31/31 [==============================] - 17s 536ms/step - loss: 0.2395 - accuracy: 0.9177 - val_loss: 0.2715 - val_accuracy: 0.9353\n","Epoch 75/100\n","31/31 [==============================] - 17s 533ms/step - loss: 0.2417 - accuracy: 0.9103 - val_loss: 0.2753 - val_accuracy: 0.9382\n","Epoch 76/100\n","31/31 [==============================] - 16s 528ms/step - loss: 0.2282 - accuracy: 0.9305 - val_loss: 0.2766 - val_accuracy: 0.9353\n","Epoch 77/100\n","31/31 [==============================] - 17s 535ms/step - loss: 0.2152 - accuracy: 0.9236 - val_loss: 0.2857 - val_accuracy: 0.9412\n","Epoch 78/100\n","31/31 [==============================] - 17s 536ms/step - loss: 0.2036 - accuracy: 0.9359 - val_loss: 0.2663 - val_accuracy: 0.9441\n","Epoch 79/100\n","31/31 [==============================] - 17s 539ms/step - loss: 0.2450 - accuracy: 0.9066 - val_loss: 0.2779 - val_accuracy: 0.9324\n","Epoch 80/100\n","31/31 [==============================] - 17s 534ms/step - loss: 0.2342 - accuracy: 0.9123 - val_loss: 0.2784 - val_accuracy: 0.9353\n","Epoch 81/100\n","31/31 [==============================] - 17s 536ms/step - loss: 0.2237 - accuracy: 0.9181 - val_loss: 0.2772 - val_accuracy: 0.9353\n","Epoch 82/100\n","31/31 [==============================] - 17s 539ms/step - loss: 0.2514 - accuracy: 0.8985 - val_loss: 0.2710 - val_accuracy: 0.9412\n","Epoch 83/100\n","31/31 [==============================] - 17s 538ms/step - loss: 0.1753 - accuracy: 0.9318 - val_loss: 0.2919 - val_accuracy: 0.9382\n","Epoch 84/100\n","31/31 [==============================] - 17s 532ms/step - loss: 0.2336 - accuracy: 0.9161 - val_loss: 0.2777 - val_accuracy: 0.9412\n","Epoch 85/100\n","31/31 [==============================] - 17s 535ms/step - loss: 0.1879 - accuracy: 0.9392 - val_loss: 0.2665 - val_accuracy: 0.9471\n","Epoch 86/100\n","31/31 [==============================] - 17s 533ms/step - loss: 0.2151 - accuracy: 0.9244 - val_loss: 0.2808 - val_accuracy: 0.9412\n","Epoch 87/100\n","31/31 [==============================] - 17s 533ms/step - loss: 0.2021 - accuracy: 0.9322 - val_loss: 0.2656 - val_accuracy: 0.9441\n","Epoch 88/100\n","31/31 [==============================] - 16s 529ms/step - loss: 0.1745 - accuracy: 0.9320 - val_loss: 0.2507 - val_accuracy: 0.9412\n","Epoch 89/100\n","31/31 [==============================] - 17s 530ms/step - loss: 0.2114 - accuracy: 0.9198 - val_loss: 0.2514 - val_accuracy: 0.9412\n","Epoch 90/100\n","31/31 [==============================] - 16s 527ms/step - loss: 0.2346 - accuracy: 0.9218 - val_loss: 0.2577 - val_accuracy: 0.9412\n","Epoch 91/100\n","31/31 [==============================] - 17s 532ms/step - loss: 0.1612 - accuracy: 0.9426 - val_loss: 0.2717 - val_accuracy: 0.9441\n","Epoch 92/100\n","31/31 [==============================] - 17s 538ms/step - loss: 0.1755 - accuracy: 0.9366 - val_loss: 0.2709 - val_accuracy: 0.9353\n","Epoch 93/100\n","31/31 [==============================] - 17s 534ms/step - loss: 0.2255 - accuracy: 0.9082 - val_loss: 0.2531 - val_accuracy: 0.9412\n","Epoch 94/100\n","31/31 [==============================] - 17s 531ms/step - loss: 0.2034 - accuracy: 0.9209 - val_loss: 0.2651 - val_accuracy: 0.9441\n","Epoch 95/100\n","31/31 [==============================] - 17s 534ms/step - loss: 0.1792 - accuracy: 0.9368 - val_loss: 0.2600 - val_accuracy: 0.9441\n","Epoch 96/100\n","31/31 [==============================] - 17s 530ms/step - loss: 0.2295 - accuracy: 0.9030 - val_loss: 0.2691 - val_accuracy: 0.9441\n","Epoch 97/100\n","31/31 [==============================] - 16s 525ms/step - loss: 0.1700 - accuracy: 0.9372 - val_loss: 0.2786 - val_accuracy: 0.9382\n","Epoch 98/100\n","31/31 [==============================] - 16s 523ms/step - loss: 0.2116 - accuracy: 0.9227 - val_loss: 0.2751 - val_accuracy: 0.9471\n","Epoch 99/100\n","31/31 [==============================] - 16s 527ms/step - loss: 0.2131 - accuracy: 0.9264 - val_loss: 0.2745 - val_accuracy: 0.9441\n","Epoch 100/100\n","31/31 [==============================] - 16s 527ms/step - loss: 0.2102 - accuracy: 0.9232 - val_loss: 0.2621 - val_accuracy: 0.9441\n","[INFO] evaluating after fine-tuning...\n","              precision    recall  f1-score   support\n","\n","    bluebell       0.90      0.90      0.90        20\n","   buttercup       1.00      1.00      1.00        19\n","   coltsfoot       0.87      0.87      0.87        15\n","     cowslip       0.95      0.90      0.93        21\n","      crocus       1.00      0.95      0.97        19\n","    daffodil       0.91      0.87      0.89        23\n","       daisy       1.00      1.00      1.00        27\n","   dandelion       0.95      0.90      0.92        20\n","  fritillary       1.00      1.00      1.00        16\n","        iris       1.00      0.95      0.98        22\n","  lilyvalley       0.94      1.00      0.97        16\n","       pansy       0.95      0.95      0.95        20\n","    snowdrop       0.90      0.95      0.93        20\n","   sunflower       1.00      1.00      1.00        18\n","   tigerlily       1.00      1.00      1.00        18\n","       tulip       0.74      0.87      0.80        23\n","  windflower       1.00      0.96      0.98        23\n","\n","    accuracy                           0.94       340\n","   macro avg       0.95      0.95      0.95       340\n","weighted avg       0.95      0.94      0.94       340\n","\n","[INFO] serializing model...\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"cvP4NEgzuO8M"},"source":[""],"execution_count":null,"outputs":[]}]}