{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"32-deep-dream.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1S4gwyfVntNS4OSN9VPYjfprq8TSJMiTT","authorship_tag":"ABX9TyP+65toJmEesmD2XhqIU3PH"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"mhockRqFm0Qf"},"source":["Setup"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1ljuJ4WBm0b0","executionInfo":{"status":"ok","timestamp":1618911312992,"user_tz":240,"elapsed":470,"user":{"displayName":"Sabina Chen","photoUrl":"","userId":"13519457631091889537"}},"outputId":"0d95bb82-fcde-49ee-8e78-89640d2c739d"},"source":["!cat ~/.keras/keras.json"],"execution_count":15,"outputs":[{"output_type":"stream","text":["{\n","    \"epsilon\": 1e-07, \n","    \"floatx\": \"float32\", \n","    \"image_data_format\": \"channels_last\", \n","    \"backend\": \"tensorflow\"\n","}"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wUvlU-LWm2T6","executionInfo":{"status":"ok","timestamp":1618911313144,"user_tz":240,"elapsed":377,"user":{"displayName":"Sabina Chen","photoUrl":"","userId":"13519457631091889537"}},"outputId":"671aff79-80a2-4981-a0e1-22b1d18942e4"},"source":["import keras\n","print(keras.__version__)"],"execution_count":16,"outputs":[{"output_type":"stream","text":["2.4.3\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"CzKn1Q1um0i7"},"source":["Imports"]},{"cell_type":"code","metadata":{"id":"n_9CP924m0rK","executionInfo":{"status":"ok","timestamp":1618911314931,"user_tz":240,"elapsed":1547,"user":{"displayName":"Sabina Chen","photoUrl":"","userId":"13519457631091889537"}}},"source":["# import the necessary packages\n","from keras.applications import InceptionV3\n","from keras.applications.inception_v3 import preprocess_input\n","from keras.preprocessing.image import img_to_array\n","from keras.preprocessing.image import load_img\n","from keras import backend as K\n","from scipy import ndimage\n","import numpy as np\n","import cv2"],"execution_count":17,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7PkKLa_8m_-i"},"source":["Helper Functions"]},{"cell_type":"code","metadata":{"id":"P6P12dn-nAF8","executionInfo":{"status":"ok","timestamp":1618912196510,"user_tz":240,"elapsed":495,"user":{"displayName":"Sabina Chen","photoUrl":"","userId":"13519457631091889537"}}},"source":["def preprocess(p):\n","\t# load the input image, convert it to a Keras-compatible array,\n","\t# expand the dimensions so we can pass it through the model, and\n","\t# then finally preprocess it for input to the Inception network\n","\timage = load_img(p)\n","\timage = img_to_array(image)\n","\timage = np.expand_dims(image, axis=0)\n","\timage = preprocess_input(image)\n","\n","\t# return the preprocessed image\n","\treturn image\n","\n","def deprocess(image):\n","\t# we are using \"channels last\" ordering so ensure the RGB\n","\t# channels are the last dimension in the matrix\n","\timage = image.reshape((image.shape[1], image.shape[2], 3))\n","\n","\t# \"undo\" the preprocessing done for Inception to bring the image\n","\t# back into the range [0, 255]\n","\timage /= 2.0\n","\timage += 0.5\n","\timage *= 255.0\n","\timage = np.clip(image, 0, 255).astype(\"uint8\")\n","\n","\t# we have been processing images in RGB order; however, OpenCV\n","\t# assumes images are in BGR order\n","\timage = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n","\n","\t# return the deprocessed image\n","\treturn image\n","\n","def resize_image(image, size):\n","\t# resize the image\n","\tresized = np.copy(image)\n","\tresized = ndimage.zoom(resized,\n","\t\t(1, float(size[0]) / resized.shape[1],\n","\t\tfloat(size[1]) / resized.shape[2], 1), order=1)\n","\n","\t# return the resized image\n","\treturn resized\n","\n","def eval_loss_and_gradients(X):\n","\t# fetch the loss and gradients given the input\n","\toutput = fetchLossGrads([X])\n","\t(loss, G) = (output[0], output[1])\n","\n","\t# return a tuple of the loss and gradients\n","\treturn (loss, G)\n","\n","def gradient_ascent(X, iters, alpha, maxLoss=-np.inf):\n","\t# loop over our number of iterations\n","\tfor i in range(0, iters):\n","\t\t# compute the loss and gradient\n","\t\t(loss, G) = eval_loss_and_gradients(X)\n","\n","\t\t# if the loss is greater than the max loss, break from the\n","\t\t# loop early to prevent strange effects\n","\t\tif loss > maxLoss:\n","\t\t\tbreak\n","\n","\t\t# take a step\n","\t\tprint(\"[INFO] Loss at {}: {}\".format(i, loss))\n","\t\tX += alpha * G\n","\n","\t# return the output of gradient ascent\n","\treturn X"],"execution_count":34,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JLi8T4SEnANl"},"source":["Deep Dream"]},{"cell_type":"code","metadata":{"id":"Y9u1XcXUrvok","executionInfo":{"status":"ok","timestamp":1618912199312,"user_tz":240,"elapsed":460,"user":{"displayName":"Sabina Chen","photoUrl":"","userId":"13519457631091889537"}}},"source":["INPUT_FILEPATH = \"drive/MyDrive/pyimagesearch/datasets/jp.jpg\"\n","OUTPUT_FILEPATH = \"drive/MyDrive/pyimagesearch/output/32-deep-dream.png\""],"execution_count":35,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_JZSFHYQnAVJ","executionInfo":{"status":"ok","timestamp":1618912271829,"user_tz":240,"elapsed":72567,"user":{"displayName":"Sabina Chen","photoUrl":"","userId":"13519457631091889537"}},"outputId":"1b40eaf8-f31b-47c3-bb97-6533d2bc7502"},"source":["# define the dictionary that includes (1) the layers we are going\n","# to use for the dream and (2) their respective weights (i.e., the\n","# larger the weight, the more the layer contributes to the dream)\n","LAYERS = {\n","    \"mixed2\": 2.0,\n","    \"mixed3\": 0.5,\n","}\n","\n","# define the number of octaves, octave scale, alpha (step for\n","# gradient ascent) number of iterations, and max loss -- tweaking\n","# these values will produce different dreams\n","NUM_OCTAVE = 3\n","OCTAVE_SCALE = 1.4\n","ALPHA = 0.001\n","NUM_ITER = 50\n","MAX_LOSS = 10.0\n","\n","# indicate that Keras *should not* be update the weights of any\n","# layer during the deep dream\n","K.set_learning_phase(0)\n","\n","# load the (pre-trained) Inception model from disk, then grab a\n","# reference variable to the input tensor of the model (which we'll\n","# then be using to perform our CNN hallucination)\n","print(\"[INFO] loading inception network...\")\n","model = InceptionV3(weights=\"imagenet\", include_top=False)\n","dream = model.input\n","\n","# define our loss value, then build a dictionary that maps the\n","# *name* of each layer inside of Inception to the actual *layer*\n","# object itself -- we'll need this mapping when building the loss\n","# of the dream\n","loss = K.variable(0.0)\n","layerMap = {layer.name: layer for layer in model.layers}\n","\n","# loop over the layers that will be utilized in the dream\n","for layerName in LAYERS:\n","    # grab the output of the layer we will use for dreaming, then add\n","    # the L2-norm of the features to the layer to the loss (we use\n","    # array slicing here to avoid border artifacts caused by border\n","    # pixels)\n","    x = layerMap[layerName].output\n","    coeff = LAYERS[layerName]\n","    scaling = K.prod(K.cast(K.shape(x), \"float32\"))\n","    loss = loss + (coeff * K.sum(K.square(x[:, 2: -2, 2: -2, :])) / scaling)\n","\n","# compute the gradients of the dream with respect to loss and then\n","# normalize\n","grads = K.gradients(loss, dream)[0]\n","grads /= K.maximum(K.mean(K.abs(grads)), 1e-7)\n","\n","# we now need to define a function that can retrieve the value of the\n","# loss and gradients given an input image\n","outputs = [loss, grads]\n","fetchLossGrads = K.function([dream], outputs)\n","\n","# load and preprocess the input image, then grab the (original) input\n","# height and width\n","image = preprocess(INPUT_FILEPATH)\n","dims = image.shape[1:3]\n","\n","# in order to perform deep dreaming we need to build multiple scales\n","# of the input image (i.e., set of images at lower and lower\n","# resolutions) -- this list stores the spatial dimensions that we\n","# will be resizing our input image to\n","octaveDims = [dims]\n","\n","# here we loop over the number of octaves (resolutions) we are going\n","# to generate\n","for i in range(1, NUM_OCTAVE):\n","    # compute the spatial dimensions (i.e., width and height) for the\n","    # current octave, then update the dimensions list\n","    size = [int(d / (OCTAVE_SCALE ** i)) for d in dims]\n","    octaveDims.append(size)\n","\n","# reverse the octave dimensions list so that the *smallest*\n","# dimensions are at the *front* of the list\n","octaveDims = octaveDims[::-1]\n","\n","# clone the original image and then create a resized input image that\n","# matches the smallest dimensions\n","orig = np.copy(image)\n","shrunk = resize_image(image, octaveDims[0])\n","\n","# loop over the ocative dimensions from smallest to largest\n","for (o, size) in enumerate(octaveDims):\n","    # resize the image and then apply gradient ascent\n","    print(\"[INFO] starting octave {}...\".format(o))\n","    image = resize_image(image, size)\n","    image = gradient_ascent(image, iters=NUM_ITER, alpha=ALPHA, maxLoss=MAX_LOSS)\n","\n","    # to compute the lost detail we need two images: (1) the shrunk\n","    # image that has been upscaled to the current octave and (2) the\n","    # original image that has been downscaled to the current octave\n","    upscaled = resize_image(shrunk, size)\n","    downscaled = resize_image(orig, size)\n","\n","    # the lost detail is computed via a simple subtraction which we\n","    # immediately back in to the image we applied gradient ascent to\n","    lost = downscaled - upscaled\n","    image += lost\n","\n","    # make the original image be the new shrunk image so we can\n","    # repeat the process\n","    shrunk = resize_image(orig, size)\n","\n","# deprocess our dream and save it to disk\n","image = deprocess(image)\n","cv2.imwrite(OUTPUT_FILEPATH, image)"],"execution_count":36,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/backend.py:434: UserWarning: `tf.keras.backend.set_learning_phase` is deprecated and will be removed after 2020-10-11. To update it, simply pass a True/False value to the `training` argument of the `__call__` method of your layer or model.\n","  warnings.warn('`tf.keras.backend.set_learning_phase` is deprecated and '\n"],"name":"stderr"},{"output_type":"stream","text":["[INFO] loading inception network...\n","[INFO] starting octave 0...\n","[INFO] Loss at 0: 0.8034265041351318\n","[INFO] Loss at 1: 0.8713101148605347\n","[INFO] Loss at 2: 0.93665611743927\n","[INFO] Loss at 3: 0.9970346689224243\n","[INFO] Loss at 4: 1.0549167394638062\n","[INFO] Loss at 5: 1.1114245653152466\n","[INFO] Loss at 6: 1.1650549173355103\n","[INFO] Loss at 7: 1.2169930934906006\n","[INFO] Loss at 8: 1.2688486576080322\n","[INFO] Loss at 9: 1.3184586763381958\n","[INFO] Loss at 10: 1.3651076555252075\n","[INFO] Loss at 11: 1.4122278690338135\n","[INFO] Loss at 12: 1.4564568996429443\n","[INFO] Loss at 13: 1.500680923461914\n","[INFO] Loss at 14: 1.5436090230941772\n","[INFO] Loss at 15: 1.5853068828582764\n","[INFO] Loss at 16: 1.6251384019851685\n","[INFO] Loss at 17: 1.6656785011291504\n","[INFO] Loss at 18: 1.7041549682617188\n","[INFO] Loss at 19: 1.7416013479232788\n","[INFO] Loss at 20: 1.7792303562164307\n","[INFO] Loss at 21: 1.815454125404358\n","[INFO] Loss at 22: 1.8519326448440552\n","[INFO] Loss at 23: 1.8881707191467285\n","[INFO] Loss at 24: 1.9223389625549316\n","[INFO] Loss at 25: 1.9564225673675537\n","[INFO] Loss at 26: 1.988762378692627\n","[INFO] Loss at 27: 2.0231776237487793\n","[INFO] Loss at 28: 2.0538506507873535\n","[INFO] Loss at 29: 2.0880258083343506\n","[INFO] Loss at 30: 2.1189277172088623\n","[INFO] Loss at 31: 2.15034818649292\n","[INFO] Loss at 32: 2.1802706718444824\n","[INFO] Loss at 33: 2.211376667022705\n","[INFO] Loss at 34: 2.2372984886169434\n","[INFO] Loss at 35: 2.26861572265625\n","[INFO] Loss at 36: 2.295851707458496\n","[INFO] Loss at 37: 2.325321912765503\n","[INFO] Loss at 38: 2.352829933166504\n","[INFO] Loss at 39: 2.3812084197998047\n","[INFO] Loss at 40: 2.4095849990844727\n","[INFO] Loss at 41: 2.435863494873047\n","[INFO] Loss at 42: 2.4607291221618652\n","[INFO] Loss at 43: 2.4896655082702637\n","[INFO] Loss at 44: 2.5137147903442383\n","[INFO] Loss at 45: 2.5417890548706055\n","[INFO] Loss at 46: 2.565889596939087\n","[INFO] Loss at 47: 2.5911903381347656\n","[INFO] Loss at 48: 2.617547035217285\n","[INFO] Loss at 49: 2.6409084796905518\n","[INFO] starting octave 1...\n","[INFO] Loss at 0: 1.1286563873291016\n","[INFO] Loss at 1: 1.236882209777832\n","[INFO] Loss at 2: 1.3341540098190308\n","[INFO] Loss at 3: 1.420678973197937\n","[INFO] Loss at 4: 1.4991079568862915\n","[INFO] Loss at 5: 1.5716465711593628\n","[INFO] Loss at 6: 1.6386367082595825\n","[INFO] Loss at 7: 1.7027390003204346\n","[INFO] Loss at 8: 1.7611231803894043\n","[INFO] Loss at 9: 1.8169567584991455\n","[INFO] Loss at 10: 1.8711951971054077\n","[INFO] Loss at 11: 1.9232536554336548\n","[INFO] Loss at 12: 1.9750943183898926\n","[INFO] Loss at 13: 2.0219945907592773\n","[INFO] Loss at 14: 2.0701708793640137\n","[INFO] Loss at 15: 2.1142125129699707\n","[INFO] Loss at 16: 2.157691478729248\n","[INFO] Loss at 17: 2.1988320350646973\n","[INFO] Loss at 18: 2.2390451431274414\n","[INFO] Loss at 19: 2.278968334197998\n","[INFO] Loss at 20: 2.317746162414551\n","[INFO] Loss at 21: 2.3546297550201416\n","[INFO] Loss at 22: 2.3914794921875\n","[INFO] Loss at 23: 2.426537036895752\n","[INFO] Loss at 24: 2.461000919342041\n","[INFO] Loss at 25: 2.4963765144348145\n","[INFO] Loss at 26: 2.5308425426483154\n","[INFO] Loss at 27: 2.5638136863708496\n","[INFO] Loss at 28: 2.5961437225341797\n","[INFO] Loss at 29: 2.6283645629882812\n","[INFO] Loss at 30: 2.6584362983703613\n","[INFO] Loss at 31: 2.690378427505493\n","[INFO] Loss at 32: 2.721318244934082\n","[INFO] Loss at 33: 2.751410961151123\n","[INFO] Loss at 34: 2.7810401916503906\n","[INFO] Loss at 35: 2.8107848167419434\n","[INFO] Loss at 36: 2.83980131149292\n","[INFO] Loss at 37: 2.867629051208496\n","[INFO] Loss at 38: 2.895554780960083\n","[INFO] Loss at 39: 2.923039197921753\n","[INFO] Loss at 40: 2.9486846923828125\n","[INFO] Loss at 41: 2.9772684574127197\n","[INFO] Loss at 42: 3.003465175628662\n","[INFO] Loss at 43: 3.0298819541931152\n","[INFO] Loss at 44: 3.055265426635742\n","[INFO] Loss at 45: 3.0813188552856445\n","[INFO] Loss at 46: 3.1064016819000244\n","[INFO] Loss at 47: 3.1301326751708984\n","[INFO] Loss at 48: 3.155120611190796\n","[INFO] Loss at 49: 3.1786916255950928\n","[INFO] starting octave 2...\n","[INFO] Loss at 0: 1.177064299583435\n","[INFO] Loss at 1: 1.2858814001083374\n","[INFO] Loss at 2: 1.384077787399292\n","[INFO] Loss at 3: 1.4725580215454102\n","[INFO] Loss at 4: 1.5549192428588867\n","[INFO] Loss at 5: 1.6305527687072754\n","[INFO] Loss at 6: 1.7036784887313843\n","[INFO] Loss at 7: 1.7719764709472656\n","[INFO] Loss at 8: 1.837203025817871\n","[INFO] Loss at 9: 1.899071216583252\n","[INFO] Loss at 10: 1.9585479497909546\n","[INFO] Loss at 11: 2.0134902000427246\n","[INFO] Loss at 12: 2.0681211948394775\n","[INFO] Loss at 13: 2.1195383071899414\n","[INFO] Loss at 14: 2.1703364849090576\n","[INFO] Loss at 15: 2.2184560298919678\n","[INFO] Loss at 16: 2.265291213989258\n","[INFO] Loss at 17: 2.310213565826416\n","[INFO] Loss at 18: 2.353834390640259\n","[INFO] Loss at 19: 2.3961503505706787\n","[INFO] Loss at 20: 2.4386301040649414\n","[INFO] Loss at 21: 2.4782168865203857\n","[INFO] Loss at 22: 2.517918586730957\n","[INFO] Loss at 23: 2.5552241802215576\n","[INFO] Loss at 24: 2.5923500061035156\n","[INFO] Loss at 25: 2.6281895637512207\n","[INFO] Loss at 26: 2.663628578186035\n","[INFO] Loss at 27: 2.698031425476074\n","[INFO] Loss at 28: 2.7326266765594482\n","[INFO] Loss at 29: 2.7646002769470215\n","[INFO] Loss at 30: 2.797635078430176\n","[INFO] Loss at 31: 2.828981637954712\n","[INFO] Loss at 32: 2.861724853515625\n","[INFO] Loss at 33: 2.8926026821136475\n","[INFO] Loss at 34: 2.922285795211792\n","[INFO] Loss at 35: 2.9536666870117188\n","[INFO] Loss at 36: 2.982573986053467\n","[INFO] Loss at 37: 3.0119643211364746\n","[INFO] Loss at 38: 3.0413618087768555\n","[INFO] Loss at 39: 3.0698647499084473\n","[INFO] Loss at 40: 3.098207950592041\n","[INFO] Loss at 41: 3.125694751739502\n","[INFO] Loss at 42: 3.152993679046631\n","[INFO] Loss at 43: 3.1805319786071777\n","[INFO] Loss at 44: 3.2074735164642334\n","[INFO] Loss at 45: 3.233365058898926\n","[INFO] Loss at 46: 3.2588484287261963\n","[INFO] Loss at 47: 3.285134792327881\n","[INFO] Loss at 48: 3.3099849224090576\n","[INFO] Loss at 49: 3.335299015045166\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":36}]}]}