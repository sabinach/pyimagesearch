{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"27-dogs-vs-cats-alexnet.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm","mount_file_id":"16gzrlavXdm99P2WltGBLy4vtQa_AYvvK","authorship_tag":"ABX9TyNb+1AE6EHqUyT6J6E6cNOB"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"VJf0YDdar2aU"},"source":["**Use GPU: Runtime -> Change runtime type -> GPU (Hardware Accelerator)**"]},{"cell_type":"markdown","metadata":{"id":"famuadZLr2t0"},"source":["Setup"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N3CWpEDFr3Jg","executionInfo":{"status":"ok","timestamp":1618891750840,"user_tz":240,"elapsed":1032,"user":{"displayName":"Sabina Chen","photoUrl":"","userId":"13519457631091889537"}},"outputId":"c3bec967-e0a7-4ffe-a53b-4885f3c90939"},"source":["!cat ~/.keras/keras.json"],"execution_count":1,"outputs":[{"output_type":"stream","text":["{\n","    \"epsilon\": 1e-07, \n","    \"floatx\": \"float32\", \n","    \"image_data_format\": \"channels_last\", \n","    \"backend\": \"tensorflow\"\n","}"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KTMIoWAijAbK","executionInfo":{"status":"ok","timestamp":1618893084346,"user_tz":240,"elapsed":847,"user":{"displayName":"Sabina Chen","photoUrl":"","userId":"13519457631091889537"}},"outputId":"785e43e0-6307-4107-ce80-1fdefd934383"},"source":["gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","if gpu_info.find('failed') >= 0:\n","  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n","  print('and then re-execute this cell.')\n","else:\n","  print(gpu_info)"],"execution_count":10,"outputs":[{"output_type":"stream","text":["Tue Apr 20 04:31:23 2021       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.67       Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   48C    P0    38W / 250W |   4715MiB / 16280MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UaHhNpsWjGAI","executionInfo":{"status":"ok","timestamp":1618893105638,"user_tz":240,"elapsed":438,"user":{"displayName":"Sabina Chen","photoUrl":"","userId":"13519457631091889537"}},"outputId":"f51b9a24-d32d-4946-b972-3ffbd238fffd"},"source":["from psutil import virtual_memory\n","ram_gb = virtual_memory().total / 1e9\n","print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n","\n","if ram_gb < 20:\n","  print('To enable a high-RAM runtime, select the Runtime > \"Change runtime type\"')\n","  print('menu, and then select High-RAM in the Runtime shape dropdown. Then, ')\n","  print('re-execute this cell.')\n","else:\n","  print('You are using a high-RAM runtime!')"],"execution_count":11,"outputs":[{"output_type":"stream","text":["Your runtime has 27.4 gigabytes of available RAM\n","\n","You are using a high-RAM runtime!\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"w9zXktkazYPf"},"source":["Kaggle Dogs-vs-Cats Config"]},{"cell_type":"code","metadata":{"id":"sy2eG8rjzYmE","executionInfo":{"status":"ok","timestamp":1618891758420,"user_tz":240,"elapsed":950,"user":{"displayName":"Sabina Chen","photoUrl":"","userId":"13519457631091889537"}}},"source":["# define the paths to the images directory\n","IMAGES_PATH = \"drive/MyDrive/pyimagesearch/datasets/kaggle-dogs-vs-cats/train\" \n","\n","# since we do not have validation data or access to the testing\n","# labels we need to take a number of images from the training\n","# data and use them instead\n","NUM_CLASSES = 2\n","NUM_VAL_IMAGES = 1250 * NUM_CLASSES\n","NUM_TEST_IMAGES = 1250 * NUM_CLASSES\n","\n","# define the path to the output training, validation, and testing\n","# HDF5 files\n","TRAIN_HDF5 = \"drive/MyDrive/pyimagesearch/output/26-kaggle-dogs-vs-cats/hdf5/train.hdf5\"\n","VAL_HDF5 = \"drive/MyDrive/pyimagesearch/output/26-kaggle-dogs-vs-cats/hdf5/val.hdf5\"\n","TEST_HDF5 = \"drive/MyDrive/pyimagesearch/output/26-kaggle-dogs-vs-cats/hdf5/test.hdf5\"\n","\n","# path to the output model file\n","MODEL_PATH = \"drive/MyDrive/pyimagesearch/output/26-kaggle-dogs-vs-cats/alexnet_dogs_vs_cats.model\"\n","\n","# define the path to the dataset mean\n","DATASET_MEAN = \"drive/MyDrive/pyimagesearch/output/26-kaggle-dogs-vs-cats/dogs_vs_cats_mean.json\"\n","\n","# define the path to the output directory used for storing plots,\n","# classification reports, etc.\n","OUTPUT_PATH = \"drive/MyDrive/pyimagesearch/output/26-kaggle-dogs-vs-cats/\""],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LIBGHeCQr5lg"},"source":["Preprocessing"]},{"cell_type":"code","metadata":{"id":"T6qojYAu0GCl","executionInfo":{"status":"ok","timestamp":1618891753959,"user_tz":240,"elapsed":3355,"user":{"displayName":"Sabina Chen","photoUrl":"","userId":"13519457631091889537"}}},"source":["# import the necessary packages\n","import numpy as np\n","import cv2\n","from sklearn.feature_extraction.image import extract_patches_2d\n","from keras.preprocessing.image import img_to_array\n","\n","class SimplePreprocessor:\n","    def __init__(self, width, height, inter=cv2.INTER_AREA):\n","        # store the target image width, height, and interpolation\n","        # method used when resizing\n","        self.width = width\n","        self.height = height\n","        self.inter = inter\n","\n","    def preprocess(self, image):\n","        # resize the image to a fixed size, ignoring the aspect\n","        # ratio\n","        return cv2.resize(image, (self.width, self.height), interpolation=self.inter)\n","\n","class PatchPreprocessor:\n","    def __init__(self, width, height):\n","        # store the target width and height of the image\n","        self.width = width\n","        self.height = height\n","\n","    def preprocess(self, image):\n","        # extract a random crop from the image with the target width\n","        # and height\n","        return extract_patches_2d(image, (self.height, self.width), max_patches=1)[0]\n","\n","class MeanPreprocessor:\n","    def __init__(self, rMean, gMean, bMean):\n","        # store the Red, Green, and Blue channel averages across a\n","        # training set\n","        self.rMean = rMean\n","        self.gMean = gMean\n","        self.bMean = bMean\n","\n","    def preprocess(self, image):\n","        # split the image into its respective Red, Green, and Blue\n","        # channels\n","        (B, G, R) = cv2.split(image.astype(\"float32\"))\n","\n","        # subtract the means for each channel\n","        R -= self.rMean\n","        G -= self.gMean\n","        B -= self.bMean\n","\n","        # merge the channels back together and return the image\n","        return cv2.merge([B, G, R])\n","\n","class ImageToArrayPreprocessor:\n","    def __init__(self, dataFormat=None):\n","        # store the image data format\n","        self.dataFormat = dataFormat\n","\n","    def preprocess(self, image):\n","        # apply the Keras utility function that correctly rearranges\n","        # the dimensions of the image\n","        return img_to_array(image, data_format=self.dataFormat)\n","\n","class CropPreprocessor:\n","    def __init__(self, width, height, horiz=True, inter=cv2.INTER_AREA):\n","        # store the target image width, height, whether or not\n","        # horizontal flips should be included, along with the\n","        # interpolation method used when resizing\n","        self.width = width\n","        self.height = height\n","        self.horiz = horiz\n","        self.inter = inter\n","\n","    def preprocess(self, image):\n","        # initialize the list of crops\n","        crops = []\n","\n","        # grab the width and height of the image then use these\n","        # dimensions to define the corners of the image based\n","        (h, w) = image.shape[:2]\n","        coords = [\n","            [0, 0, self.width, self.height],\n","            [w - self.width, 0, w, self.height],\n","            [w - self.width, h - self.height, w, h],\n","            [0, h - self.height, self.width, h]]\n","\n","        # compute the center crop of the image as well\n","        dW = int(0.5 * (w - self.width))\n","        dH = int(0.5 * (h - self.height))\n","        coords.append([dW, dH, w - dW, h - dH])\n","\n","        # loop over the coordinates, extract each of the crops,\n","        # and resize each of them to a fixed size\n","        for (startX, startY, endX, endY) in coords:\n","            crop = image[startY:endY, startX:endX]\n","            crop = cv2.resize(crop, (self.width, self.height),\n","                interpolation=self.inter)\n","            crops.append(crop)\n","\n","        # check to see if the horizontal flips should be taken\n","        if self.horiz:\n","            # compute the horizontal mirror flips for each crop\n","            mirrors = [cv2.flip(c, 1) for c in crops]\n","            crops.extend(mirrors)\n","\n","        # return the set of crops\n","        return np.array(crops)"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"COiSpphb1zPI"},"source":["HDF5"]},{"cell_type":"code","metadata":{"id":"xvez5RC310V7","executionInfo":{"status":"ok","timestamp":1618891753959,"user_tz":240,"elapsed":2096,"user":{"displayName":"Sabina Chen","photoUrl":"","userId":"13519457631091889537"}}},"source":["# import the necessary packages\n","from keras.utils import np_utils\n","import numpy as np\n","import h5py\n","\n","class HDF5DatasetGenerator:\n","    def __init__(self, dbPath, batchSize, preprocessors=None,\n","        aug=None, binarize=True, classes=2):\n","        # store the batch size, preprocessors, and data augmentor,\n","        # whether or not the labels should be binarized, along with\n","        # the total number of classes\n","        self.batchSize = batchSize\n","        self.preprocessors = preprocessors\n","        self.aug = aug\n","        self.binarize = binarize\n","        self.classes = classes\n","\n","        # open the HDF5 database for reading and determine the total\n","        # number of entries in the database\n","        self.db = h5py.File(dbPath)\n","        self.numImages = self.db[\"labels\"].shape[0]\n","\n","    def generator(self, passes=np.inf):\n","        # initialize the epoch count\n","        epochs = 0\n","\n","        # keep looping infinitely -- the model will stop once we have\n","        # reach the desired number of epochs\n","        while epochs < passes:\n","            # loop over the HDF5 dataset\n","            for i in np.arange(0, self.numImages, self.batchSize):\n","                # extract the images and labels from the HDF dataset\n","                images = self.db[\"images\"][i: i + self.batchSize]\n","                labels = self.db[\"labels\"][i: i + self.batchSize]\n","\n","                # check to see if the labels should be binarized\n","                if self.binarize:\n","                    labels = np_utils.to_categorical(labels,\n","                        self.classes)\n","\n","                # check to see if our preprocessors are not None\n","                if self.preprocessors is not None:\n","                    # initialize the list of processed images\n","                    procImages = []\n","\n","                    # loop over the images\n","                    for image in images:\n","                        # loop over the preprocessors and apply each\n","                        # to the image\n","                        for p in self.preprocessors:\n","                            image = p.preprocess(image)\n","\n","                        # update the list of processed images\n","                        procImages.append(image)\n","\n","                    # update the images array to be the processed\n","                    # images\n","                    images = np.array(procImages)\n","\n","                # if the data augmenator exists, apply it\n","                if self.aug is not None:\n","                    (images, labels) = next(self.aug.flow(images,\n","                        labels, batch_size=self.batchSize))\n","\n","                # yield a tuple of images and labels\n","                yield (images, labels)\n","\n","            # increment the total number of epochs\n","            epochs += 1\n","\n","    def close(self):\n","        # close the database\n","        self.db.close()"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"85M7RF6X2BLY"},"source":["Training Monitor"]},{"cell_type":"code","metadata":{"id":"IA3jb18x2BgH","executionInfo":{"status":"ok","timestamp":1618894493149,"user_tz":240,"elapsed":450,"user":{"displayName":"Sabina Chen","photoUrl":"","userId":"13519457631091889537"}}},"source":["# import the necessary packages\n","from keras.callbacks import BaseLogger\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import json\n","import os\n","\n","class TrainingMonitor(BaseLogger):\n","    def __init__(self, figPath, jsonPath=None, startAt=0):\n","        # store the output path for the figure, the path to the JSON\n","        # serialized file, and the starting epoch\n","        super(TrainingMonitor, self).__init__()\n","        self.figPath = figPath\n","        self.jsonPath = jsonPath\n","        self.startAt = startAt\n","\n","    def on_train_begin(self, logs={}):\n","        # initialize the history dictionary\n","        self.H = {}\n","\n","        # if the JSON history path exists, load the training history\n","        if self.jsonPath is not None:\n","            if os.path.exists(self.jsonPath):\n","                self.H = json.loads(open(self.jsonPath).read())\n","\n","                # check to see if a starting epoch was supplied\n","                if self.startAt > 0:\n","                    # loop over the entries in the history log and\n","                    # trim any entries that are past the starting\n","                    # epoch\n","                    for k in self.H.keys():\n","                        self.H[k] = self.H[k][:self.startAt]\n","\n","    def on_epoch_end(self, epoch, logs={}):\n","        # loop over the logs and update the loss, accuracy, etc.\n","        # for the entire training process\n","        for (k, v) in logs.items():\n","            l = self.H.get(k, [])\n","            l.append(float(v))\n","            self.H[k] = l\n","\n","        # check to see if the training history should be serialized\n","        # to file\n","        if self.jsonPath is not None:\n","            f = open(self.jsonPath, \"w\")\n","            f.write(json.dumps(self.H))\n","            f.close()\n","\n","        # ensure at least two epochs have passed before plotting\n","        # (epoch starts at zero)\n","        if len(self.H[\"loss\"]) > 1:\n","            # plot the training loss and accuracy\n","            N = np.arange(0, len(self.H[\"loss\"]))\n","            plt.figure()\n","            plt.plot(N, self.H[\"loss\"], label=\"train_loss\")\n","            plt.plot(N, self.H[\"val_loss\"], label=\"val_loss\")\n","            plt.plot(N, self.H[\"accuracy\"], label=\"train_accuracy\")\n","            plt.plot(N, self.H[\"val_accuracy\"], label=\"val_accuracy\")\n","            plt.title(\"Training Loss and Accuracy [Epoch {}]\".format(len(self.H[\"loss\"])))\n","            plt.xlabel(\"Epoch #\")\n","            plt.ylabel(\"Loss/Accuracy\")\n","            plt.legend()\n","\n","            # save the figure\n","            plt.savefig(self.figPath)\n","            plt.close()"],"execution_count":13,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8Cw0x__i2ASO"},"source":["AlexNet"]},{"cell_type":"code","metadata":{"id":"gEXtMre-1_vT","executionInfo":{"status":"ok","timestamp":1618894496295,"user_tz":240,"elapsed":399,"user":{"displayName":"Sabina Chen","photoUrl":"","userId":"13519457631091889537"}}},"source":["# import the necessary packages\n","from keras.models import Sequential\n","from keras.layers.normalization import BatchNormalization\n","from keras.layers.convolutional import Conv2D\n","from keras.layers.convolutional import MaxPooling2D\n","from keras.layers.core import Activation\n","from keras.layers.core import Flatten\n","from keras.layers.core import Dropout\n","from keras.layers.core import Dense\n","from keras.regularizers import l2\n","from keras import backend as K\n","\n","class AlexNet:\n","    @staticmethod\n","    def build(width, height, depth, classes, reg=0.0002):\n","        # initialize the model along with the input shape to be\n","        # \"channels last\" and the channels dimension itself\n","        model = Sequential()\n","        inputShape = (height, width, depth)\n","        chanDim = -1\n","\n","        # if we are using \"channels first\", update the input shape\n","        # and channels dimension\n","        if K.image_data_format() == \"channels_first\":\n","            inputShape = (depth, height, width)\n","            chanDim = 1\n","\n","        # Block #1: first CONV => RELU => POOL layer set\n","        model.add(Conv2D(96, (11, 11), strides=(4, 4), input_shape=inputShape, padding=\"same\", kernel_regularizer=l2(reg)))\n","        model.add(Activation(\"relu\"))\n","        model.add(BatchNormalization(axis=chanDim))\n","        model.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2)))\n","        model.add(Dropout(0.25))\n","\n","        # Block #2: second CONV => RELU => POOL layer set\n","        model.add(Conv2D(256, (5, 5), padding=\"same\", kernel_regularizer=l2(reg)))\n","        model.add(Activation(\"relu\"))\n","        model.add(BatchNormalization(axis=chanDim))\n","        model.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2)))\n","        model.add(Dropout(0.25))\n","\n","        # Block #3: CONV => RELU => CONV => RELU => CONV => RELU\n","        model.add(Conv2D(384, (3, 3), padding=\"same\", kernel_regularizer=l2(reg)))\n","        model.add(Activation(\"relu\"))\n","        model.add(BatchNormalization(axis=chanDim))\n","        model.add(Conv2D(384, (3, 3), padding=\"same\", kernel_regularizer=l2(reg)))\n","        model.add(Activation(\"relu\"))\n","        model.add(BatchNormalization(axis=chanDim))\n","        model.add(Conv2D(256, (3, 3), padding=\"same\", kernel_regularizer=l2(reg)))\n","        model.add(Activation(\"relu\"))\n","        model.add(BatchNormalization(axis=chanDim))\n","        model.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2)))\n","        model.add(Dropout(0.25))\n","\n","        # Block #4: first set of FC => RELU layers\n","        model.add(Flatten())\n","        model.add(Dense(4096, kernel_regularizer=l2(reg)))\n","        model.add(Activation(\"relu\"))\n","        model.add(BatchNormalization())\n","        model.add(Dropout(0.5))\n","\n","        # Block #5: second set of FC => RELU layers\n","        model.add(Dense(4096, kernel_regularizer=l2(reg)))\n","        model.add(Activation(\"relu\"))\n","        model.add(BatchNormalization())\n","        model.add(Dropout(0.5))\n","\n","        # softmax classifier\n","        model.add(Dense(classes, kernel_regularizer=l2(reg)))\n","        model.add(Activation(\"softmax\"))\n","\n","        # return the constructed network architecture\n","        return model"],"execution_count":14,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gqYYf-OKzbQL"},"source":["Train AlexNet"]},{"cell_type":"code","metadata":{"id":"4suJdwsqzsvQ","executionInfo":{"status":"ok","timestamp":1618894499991,"user_tz":240,"elapsed":575,"user":{"displayName":"Sabina Chen","photoUrl":"","userId":"13519457631091889537"}}},"source":["# import the necessary packages\n","from keras.preprocessing.image import ImageDataGenerator\n","from keras.optimizers import Adam\n","import matplotlib\n","import json\n","import os"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"id":"ke9jcQmHzdLT","executionInfo":{"status":"ok","timestamp":1618894501187,"user_tz":240,"elapsed":376,"user":{"displayName":"Sabina Chen","photoUrl":"","userId":"13519457631091889537"}}},"source":["def train_alexnet_dogs_vs_cats():\n","    # construct the training image generator for data augmentation\n","    aug = ImageDataGenerator(rotation_range=20, zoom_range=0.15,\n","        width_shift_range=0.2, height_shift_range=0.2, shear_range=0.15,\n","        horizontal_flip=True, fill_mode=\"nearest\")\n","\n","    # load the RGB means for the training set\n","    means = json.loads(open(DATASET_MEAN).read())\n","\n","    # initialize the image preprocessors\n","    sp = SimplePreprocessor(227, 227)\n","    pp = PatchPreprocessor(227, 227)\n","    mp = MeanPreprocessor(means[\"R\"], means[\"G\"], means[\"B\"])\n","    iap = ImageToArrayPreprocessor()\n","\n","    # initialize the training and validation dataset generators\n","    trainGen = HDF5DatasetGenerator(TRAIN_HDF5, 128, aug=aug, preprocessors=[pp, mp, iap], classes=2)\n","    valGen = HDF5DatasetGenerator(VAL_HDF5, 128, preprocessors=[sp, mp, iap], classes=2)\n","\n","    # initialize the optimizer\n","    print(\"[INFO] compiling model...\")\n","    opt = Adam(lr=1e-3)\n","    model = AlexNet.build(width=227, height=227, depth=3, classes=2, reg=0.0002)\n","    model.compile(loss=\"binary_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n","\n","    # construct the set of callbacks\n","    path = os.path.sep.join([OUTPUT_PATH, \"{}.png\".format(os.getpid())])\n","    callbacks = [TrainingMonitor(path)]\n","\n","    # train the network\n","    model.fit_generator(\n","        trainGen.generator(),\n","        steps_per_epoch=trainGen.numImages // 128,\n","        validation_data=valGen.generator(),\n","        validation_steps=valGen.numImages // 128,\n","        epochs=75,\n","        max_queue_size=10,\n","        callbacks=callbacks, verbose=1)\n","\n","    # save the model to file\n","    print(\"[INFO] serializing model...\")\n","    model.save(MODEL_PATH, overwrite=True)\n","\n","    # close the HDF5 datasets\n","    trainGen.close()\n","    valGen.close()\n","    print(\"[INFO] done.\")"],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_cKqhGIN2ybv","outputId":"fa74570f-92f3-4933-cd63-01c2ac07800d"},"source":["train_alexnet_dogs_vs_cats()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:20: H5pyDeprecationWarning: The default file mode will change to 'r' (read-only) in h5py 3.0. To suppress this warning, pass the mode you need to h5py.File(), or set the global default h5.get_config().default_file_mode, or set the environment variable H5PY_DEFAULT_READONLY=1. Available modes are: 'r', 'r+', 'w', 'w-'/'x', 'a'. See the docs for details.\n"],"name":"stderr"},{"output_type":"stream","text":["[INFO] compiling model...\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n","  warnings.warn('`Model.fit_generator` is deprecated and '\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch 1/75\n","156/156 [==============================] - 623s 4s/step - loss: 3.7105 - accuracy: 0.5470 - val_loss: 2.8314 - val_accuracy: 0.5942\n","Epoch 2/75\n","156/156 [==============================] - 621s 4s/step - loss: 2.5192 - accuracy: 0.6189 - val_loss: 1.9226 - val_accuracy: 0.6850\n","Epoch 3/75\n","156/156 [==============================] - 620s 4s/step - loss: 1.8648 - accuracy: 0.6727 - val_loss: 1.6188 - val_accuracy: 0.6530\n","Epoch 4/75\n","156/156 [==============================] - 622s 4s/step - loss: 1.4489 - accuracy: 0.7155 - val_loss: 1.1524 - val_accuracy: 0.7800\n","Epoch 5/75\n","156/156 [==============================] - 618s 4s/step - loss: 1.1728 - accuracy: 0.7413 - val_loss: 1.0832 - val_accuracy: 0.7081\n","Epoch 6/75\n","156/156 [==============================] - 628s 4s/step - loss: 0.9880 - accuracy: 0.7840 - val_loss: 0.8450 - val_accuracy: 0.8026\n","Epoch 7/75\n","156/156 [==============================] - 628s 4s/step - loss: 0.8721 - accuracy: 0.7954 - val_loss: 0.8741 - val_accuracy: 0.7410\n","Epoch 8/75\n","156/156 [==============================] - 631s 4s/step - loss: 0.7962 - accuracy: 0.8111 - val_loss: 0.7107 - val_accuracy: 0.8306\n","Epoch 9/75\n","156/156 [==============================] - 632s 4s/step - loss: 0.7708 - accuracy: 0.8255 - val_loss: 0.7304 - val_accuracy: 0.8063\n","Epoch 10/75\n","156/156 [==============================] - 631s 4s/step - loss: 0.7109 - accuracy: 0.8363 - val_loss: 0.7202 - val_accuracy: 0.8178\n","Epoch 11/75\n","156/156 [==============================] - 633s 4s/step - loss: 0.7614 - accuracy: 0.8294 - val_loss: 0.6859 - val_accuracy: 0.8483\n","Epoch 12/75\n","156/156 [==============================] - 634s 4s/step - loss: 0.7218 - accuracy: 0.8471 - val_loss: 0.7277 - val_accuracy: 0.8257\n","Epoch 13/75\n","156/156 [==============================] - 636s 4s/step - loss: 0.7033 - accuracy: 0.8468 - val_loss: 0.6951 - val_accuracy: 0.8565\n","Epoch 14/75\n","156/156 [==============================] - 636s 4s/step - loss: 0.7693 - accuracy: 0.8454 - val_loss: 0.9159 - val_accuracy: 0.8162\n","Epoch 15/75\n","156/156 [==============================] - 640s 4s/step - loss: 0.8001 - accuracy: 0.8505 - val_loss: 0.7585 - val_accuracy: 0.8059\n","Epoch 16/75\n","156/156 [==============================] - 640s 4s/step - loss: 0.7198 - accuracy: 0.8535 - val_loss: 0.8165 - val_accuracy: 0.8072\n","Epoch 17/75\n","156/156 [==============================] - 640s 4s/step - loss: 0.6805 - accuracy: 0.8663 - val_loss: 0.9756 - val_accuracy: 0.7356\n","Epoch 18/75\n","156/156 [==============================] - 640s 4s/step - loss: 0.7129 - accuracy: 0.8709 - val_loss: 0.7687 - val_accuracy: 0.8561\n","Epoch 19/75\n","107/156 [===================>..........] - ETA: 2:59 - loss: 0.7369 - accuracy: 0.8613"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"VAXn139VeSBS"},"source":["Rank-5 Accuracy"]},{"cell_type":"code","metadata":{"id":"NVBoSxeDeTPA"},"source":["import numpy as np\n","\n","def rank5_accuracy(preds, labels):\n","\t# initialize the rank-1 and rank-5 accuracies\n","\trank1 = 0\n","\trank5 = 0\n","\n","\t# loop over the predictions and ground-truth labels\n","\tfor (p, gt) in zip(preds, labels):\n","\t\t# sort the probabilities by their index in descending\n","\t\t# order so that the more confident guesses are at the\n","\t\t# front of the list\n","\t\tp = np.argsort(p)[::-1]\n","\n","\t\t# check if the ground-truth label is in the top-5\n","\t\t# predictions\n","\t\tif gt in p[:5]:\n","\t\t\trank5 += 1\n","\n","\t\t# check to see if the ground-truth is the #1 prediction\n","\t\tif gt == p[0]:\n","\t\t\trank1 += 1\n","\n","\t# compute the final rank-1 and rank-5 accuracies\n","\trank1 /= float(len(preds))\n","\trank5 /= float(len(preds))\n","\n","\t# return a tuple of the rank-1 and rank-5 accuracies\n","\treturn (rank1, rank5)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"utBSflhlZhfn"},"source":["Crop Accuracy"]},{"cell_type":"code","metadata":{"id":"p-Urs6uMZy8H"},"source":["# import the necessary packages\n","from keras.models import load_model\n","import numpy as np\n","import progressbar\n","import json"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3vc_ZxOnef4b"},"source":["def crop_accuracy():\n","    # load the RGB means for the training set\n","    means = json.loads(open(DATASET_MEAN).read())\n","\n","    # initialize the image preprocessors\n","    sp = SimplePreprocessor(227, 227)\n","    mp = MeanPreprocessor(means[\"R\"], means[\"G\"], means[\"B\"])\n","    cp = CropPreprocessor(227, 227)\n","    iap = ImageToArrayPreprocessor()\n","\n","    # load the pretrained network\n","    print(\"[INFO] loading model...\")\n","    model = load_model(MODEL_PATH)\n","\n","    # initialize the testing dataset generator, then make predictions on\n","    # the testing data\n","    print(\"[INFO] predicting on test data (no crops)...\")\n","    testGen = HDF5DatasetGenerator(config.TEST_HDF5, 64, preprocessors=[sp, mp, iap], classes=2)\n","    predictions = model.predict_generator(testGen.generator(), steps=testGen.numImages // 64, max_queue_size=10)\n","\n","    # compute the rank-1 and rank-5 accuracies\n","    (rank1, _) = rank5_accuracy(predictions, testGen.db[\"labels\"])\n","    print(\"[INFO] rank-1: {:.2f}%\".format(rank1 * 100))\n","    testGen.close()\n","\n","    # re-initialize the testing set generator, this time excluding the\n","    # `SimplePreprocessor`\n","    testGen = HDF5DatasetGenerator(config.TEST_HDF5, 64, preprocessors=[mp], classes=2)\n","    predictions = []\n","\n","    # initialize the progress bar\n","    widgets = [\"Evaluating: \", progressbar.Percentage(), \" \", progressbar.Bar(), \" \", progressbar.ETA()]\n","    pbar = progressbar.ProgressBar(maxval=testGen.numImages // 64, widgets=widgets).start()\n","\n","    # loop over a single pass of the test data\n","    for (i, (images, labels)) in enumerate(testGen.generator(passes=1)):\n","        # loop over each of the individual images\n","        for image in images:\n","            # apply the crop preprocessor to the image to generate 10\n","            # separate crops, then convert them from images to arrays\n","            crops = cp.preprocess(image)\n","            crops = np.array([iap.preprocess(c) for c in crops], dtype=\"float32\")\n","\n","            # make predictions on the crops and then average them\n","            # together to obtain the final prediction\n","            pred = model.predict(crops)\n","            predictions.append(pred.mean(axis=0))\n","\n","        # update the progress bar\n","        pbar.update(i)\n","\n","    # compute the rank-1 accuracy\n","    pbar.finish()\n","    print(\"[INFO] predicting on test data (with crops)...\")\n","    (rank1, _) = rank5_accuracy(predictions, testGen.db[\"labels\"])\n","    print(\"[INFO] rank-1: {:.2f}%\".format(rank1 * 100))\n","    testGen.close()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mtTjtZFyfD2p"},"source":["crop_accuracy()"],"execution_count":null,"outputs":[]}]}