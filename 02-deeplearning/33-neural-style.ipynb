{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"33-neural-style.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm","mount_file_id":"1ctFRXj3ORK1wzq26S5Cn_wYXcf6m0YMe","authorship_tag":"ABX9TyPRbwc/V6QIKNQ59buycQJa"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"xIKiUQo_sKqR"},"source":["**Use GPU: Runtime -> Change runtime type -> GPU (Hardware Accelerator)**"]},{"cell_type":"markdown","metadata":{"id":"ddfUZv0AnSuy"},"source":["Setup"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"473QgPCXnS0a","executionInfo":{"status":"ok","timestamp":1619029712177,"user_tz":240,"elapsed":341,"user":{"displayName":"Sabina Chen","photoUrl":"","userId":"13519457631091889537"}},"outputId":"5fabc06d-3600-4ee1-fd29-ebd995f4d2ea"},"source":["!cat ~/.keras/keras.json"],"execution_count":1,"outputs":[{"output_type":"stream","text":["{\n","    \"epsilon\": 1e-07, \n","    \"floatx\": \"float32\", \n","    \"image_data_format\": \"channels_last\", \n","    \"backend\": \"tensorflow\"\n","}"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NoeuxY3InVpA","executionInfo":{"status":"ok","timestamp":1619029714218,"user_tz":240,"elapsed":1923,"user":{"displayName":"Sabina Chen","photoUrl":"","userId":"13519457631091889537"}},"outputId":"d52cadfd-1e9f-4143-c094-3341e20fa1dc"},"source":["import keras\n","print(keras.__version__)"],"execution_count":2,"outputs":[{"output_type":"stream","text":["2.4.3\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"BqCeMvMEnTHA"},"source":["Neural Style"]},{"cell_type":"code","metadata":{"id":"_dHJVcm9nTNQ","executionInfo":{"status":"ok","timestamp":1619029715330,"user_tz":240,"elapsed":2210,"user":{"displayName":"Sabina Chen","photoUrl":"","userId":"13519457631091889537"}}},"source":["# import the necessary packages\n","from keras.applications.vgg16 import preprocess_input\n","from keras.preprocessing.image import img_to_array\n","from keras.preprocessing.image import load_img\n","from keras import backend as K\n","from scipy.optimize import fmin_l_bfgs_b\n","import numpy as np\n","import cv2\n","import os\n","\n","class NeuralStyle:\n","    def __init__(self, settings):\n","        # store the settings dictionary\n","        self.S = settings\n","\n","        # grab the dimensions of the input image\n","        (w, h) = load_img(self.S[\"input_path\"]).size\n","        self.dims = (h, w)\n","\n","        # load content image and style images, forcing the dimensions\n","        # of our input image\n","        self.content = self.preprocess(settings[\"input_path\"])\n","        self.style = self.preprocess(settings[\"style_path\"])\n","        self.content = K.variable(self.content)\n","        self.style = K.variable(self.style)\n","\n","        # allocate memory of our output image, then combine the\n","        # content, style, and output into a single tensor so they can\n","        # be fed through the network\n","        self.output = K.placeholder((1, self.dims[0], self.dims[1], 3))\n","        self.input = K.concatenate([self.content, self.style, self.output], axis=0)\n","\n","        # load our model from disk\n","        print(\"[INFO] loading network...\")\n","        self.model = self.S[\"net\"](weights=\"imagenet\", include_top=False, input_tensor=self.input)\n","\n","        # build a dictionary that maps the *name* of each layer\n","        # inside the network to the actual layer *output*\n","        layerMap = {l.name: l.output for l in self.model.layers}\n","\n","        # extract features from the content layer, then extract the\n","        # activations from the style image (index 0) and the output\n","        # image (index 2) -- these will serve as our style features\n","        # and output features from the *content* layer\n","        contentFeatures = layerMap[self.S[\"content_layer\"]]\n","        styleFeatures = contentFeatures[0, :, :, :]\n","        outputFeatures = contentFeatures[2, :, :, :]\n","\n","        # compute the feature reconstruction loss, weighting it\n","        # appropriately\n","        contentLoss = self.featureReconLoss(styleFeatures, outputFeatures)\n","        contentLoss = contentLoss * self.S[\"content_weight\"]\n","\n","        # initialize our style loss along with the value used to\n","        # weight each style layer (in proportion to the total number\n","        # of style layers\n","        styleLoss = K.variable(0.0)\n","        weight = 1.0 / len(self.S[\"style_layers\"])\n","\n","        # loop over the style layers\n","        for layer in self.S[\"style_layers\"]:\n","            # grab the current style layer and use it to extract the\n","            # style features and output features from the *style\n","            # layer*\n","            styleOutput = layerMap[layer]\n","            styleFeatures = styleOutput[1, :, :, :]\n","            outputFeatures = styleOutput[2, :, :, :]\n","\n","            # compute the style reconstruction loss as we go\n","            T = self.styleReconLoss(styleFeatures, outputFeatures)\n","            styleLoss = styleLoss + (weight * T)\n","\n","        # finish computing the style loss, compute the total\n","        # variational loss, and then compute the total loss that\n","        # combines all three\n","        styleLoss = styleLoss * self.S[\"style_weight\"]\n","        tvLoss = self.S[\"tv_weight\"] * self.tvLoss(self.output)\n","        totalLoss = contentLoss + styleLoss + tvLoss\n","\n","        # compute the gradients out of the output image with respect\n","        # to loss\n","        grads = K.gradients(totalLoss, self.output)\n","        outputs = [totalLoss]\n","        outputs = outputs + grads\n","\n","        # the implementation of L-BFGS we will be using requires that\n","        # our loss and gradients be *two separate functions* so here\n","        # we create a Keras function that can compute both the loss\n","        # and gradients together and then return each separately\n","        # using two different class methods\n","        self.lossAndGrads = K.function([self.output], outputs)\n","\n","    def preprocess(self, p):\n","        # load the input image (while resizing it to the desired\n","        # dimensions) and preprocess it\n","        image = load_img(p, target_size=self.dims)\n","        image = img_to_array(image)\n","        image = np.expand_dims(image, axis=0)\n","        image = preprocess_input(image)\n","\n","        # return the preprocessed image\n","        return image\n","\n","    def deprocess(self, image):\n","        # reshape the image, then reverse the zero-centering by\n","        # *adding* back in the mean values across the ImageNet\n","        # training set\n","        image = image.reshape((self.dims[0], self.dims[1], 3))\n","        image[:, :, 0] += 103.939\n","        image[:, :, 1] += 116.779\n","        image[:, :, 2] += 123.680\n","\n","        # clip any values falling outside the range [0, 255] and\n","        # convert the image to an unsigned 8-bit integer\n","        image = np.clip(image, 0, 255).astype(\"uint8\")\n","\n","        # return the deprocessed image\n","        return image\n","\n","    def gramMat(self, X):\n","        # the gram matrix is the dot product between the input\n","        # vectors and their respective transpose\n","        features = K.permute_dimensions(X, (2, 0, 1))\n","        features = K.batch_flatten(features)\n","        features = K.dot(features, K.transpose(features))\n","\n","        # return the gram matrix\n","        return features\n","\n","    def featureReconLoss(self, styleFeatures, outputFeatures):\n","        # the feature reconstruction loss is the squared error\n","        # between the style features and output output features\n","        return K.sum(K.square(outputFeatures - styleFeatures))\n","\n","    def styleReconLoss(self, styleFeatures, outputFeatures):\n","        # compute the style reconstruction loss where A is the gram\n","        # matrix for the style image and G is the gram matrix for the\n","        # generated image\n","        A = self.gramMat(styleFeatures)\n","        G = self.gramMat(outputFeatures)\n","\n","        # compute the scaling factor of the style loss, then finish\n","        # computing the style reconstruction loss\n","        scale = 1.0 / float((2 * 3 * self.dims[0] * self.dims[1]) ** 2)\n","        loss = scale * K.sum(K.square(G - A))\n","\n","        # return the style reconstruction loss\n","        return loss\n","\n","    def tvLoss(self, X):\n","        # the total variational loss encourages spatial smoothness in\n","        # the output page -- here we avoid border pixels to avoid\n","        # artifacts\n","        (h, w) = self.dims\n","        A = K.square(X[:, :h - 1, :w - 1, :] - X[:, 1:, :w - 1, :])\n","        B = K.square(X[:, :h - 1, :w - 1, :] - X[:, :h - 1, 1:, :])\n","        loss = K.sum(K.pow(A + B, 1.25))\n","\n","        # return the total variational loss\n","        return loss\n","\n","    def transfer(self, maxEvals=20):\n","        # generate a random noise image that will serve as a\n","        # placeholder array, slowly modified as we run L-BFGS to\n","        # apply style transfer\n","        X = np.random.uniform(0, 255, (1, self.dims[0], self.dims[1], 3)) - 128\n","\n","        # start looping over the desired number of iterations\n","        for i in range(0, self.S[\"iterations\"]):\n","            # run L-BFGS over the pixels in our generated image to\n","            # minimize the neural style loss\n","            print(\"[INFO] starting iteration {} of {}...\".format(i + 1, self.S[\"iterations\"]))\n","            (X, loss, _) = fmin_l_bfgs_b(self.loss, X.flatten(), fprime=self.grads, maxfun=maxEvals)\n","            print(\"[INFO] end of iteration {}, loss: {:.4e}\".format(i + 1, loss))\n","\n","            # deprocess the generated image and write it to disk\n","            image = self.deprocess(X.copy())\n","            p = os.path.sep.join([self.S[\"output_path\"], \"iter_{}.png\".format(i)])\n","            cv2.imwrite(p, image)\n","\n","    def loss(self, X):\n","        # extract the loss value\n","        X = X.reshape((1, self.dims[0], self.dims[1], 3))\n","        lossValue = self.lossAndGrads([X])[0]\n","\n","        # return the loss\n","        return lossValue\n","\n","    def grads(self, X):\n","        # compute the loss and gradients\n","        X = X.reshape((1, self.dims[0], self.dims[1], 3))\n","        output = self.lossAndGrads([X])\n","\n","        # extract and return the gradient values\n","        return output[1].flatten().astype(\"float64\")"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CjycMlBxnS58"},"source":["Settings"]},{"cell_type":"code","metadata":{"id":"cnQVMArZroCX","executionInfo":{"status":"ok","timestamp":1619029715333,"user_tz":240,"elapsed":1430,"user":{"displayName":"Sabina Chen","photoUrl":"","userId":"13519457631091889537"}}},"source":["import tensorflow as tf\n","tf.compat.v1.disable_eager_execution()"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NFq1zfx5nTAU","executionInfo":{"status":"ok","timestamp":1619029890605,"user_tz":240,"elapsed":176359,"user":{"displayName":"Sabina Chen","photoUrl":"","userId":"13519457631091889537"}},"outputId":"23ad40fd-03db-4666-bbe5-b71dde0fec6a"},"source":["# import the necessary packages\n","from keras.applications import VGG19\n","\n","# initialize the settings dictionary\n","SETTINGS = {\n","\t# initialize the path to the input (i.e., content) image,\n","\t# style image, and path to the output directory\n","\t\"input_path\": \"drive/MyDrive/pyimagesearch/datasets/neural-style/jurassic_park.jpg\",\n","\t\"style_path\": \"drive/MyDrive/pyimagesearch/datasets/neural-style/mcescher.jpg\",\n","\t\"output_path\": \"drive/MyDrive/pyimagesearch/output/33-neural-style\",\n","\n","\t# define the CNN to be used style transfer, along with the\n","\t# set of content layer and style layers, respectively\n","\t\"net\": VGG19,\n","\t\"content_layer\": \"block4_conv2\",\n","\t\"style_layers\": [\"block1_conv1\", \"block2_conv1\",\n","\t\t\"block3_conv1\", \"block4_conv1\", \"block5_conv1\"],\n","\n","\t# store the content, style, and total variation weights,\n","\t# respectively\n","\t\"content_weight\": 1.0,\n","\t\"style_weight\": 100.0,\n","\t\"tv_weight\": 10.0,\n","\n","\t# number of iterations\n","\t\"iterations\": 50,\n","}\n","\n","# perform neural style transfer\n","ns = NeuralStyle(SETTINGS)\n","ns.transfer()"],"execution_count":5,"outputs":[{"output_type":"stream","text":["[INFO] loading network...\n","Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg19/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\n","80142336/80134624 [==============================] - 0s 0us/step\n","[INFO] starting iteration 1 of 50...\n","[INFO] end of iteration 1, loss: 2.6831e+12\n","[INFO] starting iteration 2 of 50...\n","[INFO] end of iteration 2, loss: 1.4092e+12\n","[INFO] starting iteration 3 of 50...\n","[INFO] end of iteration 3, loss: 8.4431e+11\n","[INFO] starting iteration 4 of 50...\n","[INFO] end of iteration 4, loss: 6.0991e+11\n","[INFO] starting iteration 5 of 50...\n","[INFO] end of iteration 5, loss: 4.9530e+11\n","[INFO] starting iteration 6 of 50...\n","[INFO] end of iteration 6, loss: 4.3392e+11\n","[INFO] starting iteration 7 of 50...\n","[INFO] end of iteration 7, loss: 3.8988e+11\n","[INFO] starting iteration 8 of 50...\n","[INFO] end of iteration 8, loss: 3.6184e+11\n","[INFO] starting iteration 9 of 50...\n","[INFO] end of iteration 9, loss: 3.4381e+11\n","[INFO] starting iteration 10 of 50...\n","[INFO] end of iteration 10, loss: 3.2868e+11\n","[INFO] starting iteration 11 of 50...\n","[INFO] end of iteration 11, loss: 3.1867e+11\n","[INFO] starting iteration 12 of 50...\n","[INFO] end of iteration 12, loss: 3.1016e+11\n","[INFO] starting iteration 13 of 50...\n","[INFO] end of iteration 13, loss: 3.0293e+11\n","[INFO] starting iteration 14 of 50...\n","[INFO] end of iteration 14, loss: 2.9722e+11\n","[INFO] starting iteration 15 of 50...\n","[INFO] end of iteration 15, loss: 2.9293e+11\n","[INFO] starting iteration 16 of 50...\n","[INFO] end of iteration 16, loss: 2.8915e+11\n","[INFO] starting iteration 17 of 50...\n","[INFO] end of iteration 17, loss: 2.8553e+11\n","[INFO] starting iteration 18 of 50...\n","[INFO] end of iteration 18, loss: 2.8249e+11\n","[INFO] starting iteration 19 of 50...\n","[INFO] end of iteration 19, loss: 2.7976e+11\n","[INFO] starting iteration 20 of 50...\n","[INFO] end of iteration 20, loss: 2.7742e+11\n","[INFO] starting iteration 21 of 50...\n","[INFO] end of iteration 21, loss: 2.7519e+11\n","[INFO] starting iteration 22 of 50...\n","[INFO] end of iteration 22, loss: 2.7322e+11\n","[INFO] starting iteration 23 of 50...\n","[INFO] end of iteration 23, loss: 2.7140e+11\n","[INFO] starting iteration 24 of 50...\n","[INFO] end of iteration 24, loss: 2.6963e+11\n","[INFO] starting iteration 25 of 50...\n","[INFO] end of iteration 25, loss: 2.6807e+11\n","[INFO] starting iteration 26 of 50...\n","[INFO] end of iteration 26, loss: 2.6665e+11\n","[INFO] starting iteration 27 of 50...\n","[INFO] end of iteration 27, loss: 2.6538e+11\n","[INFO] starting iteration 28 of 50...\n","[INFO] end of iteration 28, loss: 2.6417e+11\n","[INFO] starting iteration 29 of 50...\n","[INFO] end of iteration 29, loss: 2.6308e+11\n","[INFO] starting iteration 30 of 50...\n","[INFO] end of iteration 30, loss: 2.6199e+11\n","[INFO] starting iteration 31 of 50...\n","[INFO] end of iteration 31, loss: 2.6102e+11\n","[INFO] starting iteration 32 of 50...\n","[INFO] end of iteration 32, loss: 2.6012e+11\n","[INFO] starting iteration 33 of 50...\n","[INFO] end of iteration 33, loss: 2.5929e+11\n","[INFO] starting iteration 34 of 50...\n","[INFO] end of iteration 34, loss: 2.5850e+11\n","[INFO] starting iteration 35 of 50...\n","[INFO] end of iteration 35, loss: 2.5772e+11\n","[INFO] starting iteration 36 of 50...\n","[INFO] end of iteration 36, loss: 2.5698e+11\n","[INFO] starting iteration 37 of 50...\n","[INFO] end of iteration 37, loss: 2.5627e+11\n","[INFO] starting iteration 38 of 50...\n","[INFO] end of iteration 38, loss: 2.5559e+11\n","[INFO] starting iteration 39 of 50...\n","[INFO] end of iteration 39, loss: 2.5492e+11\n","[INFO] starting iteration 40 of 50...\n","[INFO] end of iteration 40, loss: 2.5430e+11\n","[INFO] starting iteration 41 of 50...\n","[INFO] end of iteration 41, loss: 2.5370e+11\n","[INFO] starting iteration 42 of 50...\n","[INFO] end of iteration 42, loss: 2.5314e+11\n","[INFO] starting iteration 43 of 50...\n","[INFO] end of iteration 43, loss: 2.5258e+11\n","[INFO] starting iteration 44 of 50...\n","[INFO] end of iteration 44, loss: 2.5201e+11\n","[INFO] starting iteration 45 of 50...\n","[INFO] end of iteration 45, loss: 2.5145e+11\n","[INFO] starting iteration 46 of 50...\n","[INFO] end of iteration 46, loss: 2.5096e+11\n","[INFO] starting iteration 47 of 50...\n","[INFO] end of iteration 47, loss: 2.5050e+11\n","[INFO] starting iteration 48 of 50...\n","[INFO] end of iteration 48, loss: 2.5003e+11\n","[INFO] starting iteration 49 of 50...\n","[INFO] end of iteration 49, loss: 2.4962e+11\n","[INFO] starting iteration 50 of 50...\n","[INFO] end of iteration 50, loss: 2.4926e+11\n"],"name":"stdout"}]}]}